{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Imports and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=python3.6\n",
      "env: PYSPARK_PYTHON=python3.6\n",
      "env: ARROW_PRE_0_15_IPC_FORMAT=1\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, size, isnan, array_contains, when, count, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import pathlib\n",
    "import platform\n",
    "\n",
    "#env variables \n",
    "if platform.system()=='Windows':\n",
    "    %env PYSPARK_DRIVER_PYTHON=python\n",
    "    %env PYSPARK_PYTHON=python\n",
    "elif platform.system()=='Linux':\n",
    "    %env PYSPARK_DRIVER_PYTHON=python\n",
    "    %env PYSPARK_PYTHON=python3\n",
    "else:\n",
    "    %env PYSPARK_DRIVER_PYTHON=python3.6 \n",
    "    %env PYSPARK_PYTHON=python3.6 \n",
    "\n",
    "#incompatibility with Pyarrow\n",
    "#need to install Pyarrow 0.14.1 or lower or Set the environment variable ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "%env ARROW_PRE_0_15_IPC_FORMAT=1 \n",
    "\n",
    "#used versions: \n",
    "#spark='2.4.3' python='3.6' pyarrow='0.14.1' \n",
    "\n",
    "#for new system:\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#%pip install numpy\n",
    "#%pip install -U matplotlib\n",
    "#%pip install pandas\n",
    "#%pip install Pyarrow==0.14.0\n",
    "#%env PYSPARK_DRIVER_PYTHON=python\n",
    "#%env PYSPARK_PYTHON=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "\n",
    "BASE_PATH=pathlib.Path().absolute()\n",
    "\n",
    "KMEANS_REL_PATH=\"kmeans models\"\n",
    "DATASET_REL_PATH=\"dataset\"\n",
    "\n",
    "\n",
    "DATASET_PATH=os.path.join(BASE_PATH,DATASET_REL_PATH)\n",
    "KMEANS_PATH=os.path.join(BASE_PATH,KMEANS_REL_PATH)\n",
    "\n",
    "#print(DATASET_PATH)\n",
    "#print(KMEANS_PATH)\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Load/Save Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "#load and save .read_pickle() and .to_pickle() \n",
    "\n",
    "#save\n",
    "#dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "#aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "#dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "#aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "#json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "#aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#load\n",
    "dataset=pd.read_pickle(os.path.join(DATASET_PATH, 'dataset.pkl'))\n",
    "aggregated_dataset=pd.read_pickle(os.path.join(DATASET_PATH, 'aggregated_dataset.pkl'))\n",
    "json_dataset=pd.read_pickle(os.path.join(DATASET_PATH, 'json_dataset.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Malicious Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate malicious samples\n",
    "def h1(x):\n",
    "    MAX=0.8\n",
    "    MIN=0.1\n",
    "    alpha=random.uniform(MIN,MAX)\n",
    "    temp=np.array(x)\n",
    "    return (temp*alpha).tolist()\n",
    "\n",
    "def h2(x):\n",
    "    MIN_OFF=4 # hour\n",
    "    DURATION=random.randint(MIN_OFF,23)\n",
    "    START=random.randint(0,23-DURATION) if DURATION!=23 else 0\n",
    "    END=START+DURATION\n",
    "    temp=[]\n",
    "    for i in range(len(x)):\n",
    "        if i<START or i>=END:\n",
    "            temp.append(x[i])\n",
    "        else:\n",
    "            temp.append(0.0)\n",
    "    return temp\n",
    "    \n",
    "def h3(x):\n",
    "    MAX=0.8\n",
    "    MIN=0.1\n",
    "    temp=[]\n",
    "    for i in range(len(x)):\n",
    "        temp.append(x[i]*random.uniform(MIN,MAX))\n",
    "    return temp\n",
    "\n",
    "def h4(x):\n",
    "    MAX=0.8\n",
    "    MIN=0.1\n",
    "    mean=np.mean(x)\n",
    "    temp=[]\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean*random.uniform(MIN,MAX))\n",
    "    return temp\n",
    "\n",
    "def h5(x):\n",
    "    MAX=0.8\n",
    "    MIN=0.1\n",
    "    mean=np.mean(x)\n",
    "    temp=[]\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean)\n",
    "    return temp\n",
    "\n",
    "def h6(x):\n",
    "    temp=np.array(x)\n",
    "    #temp=temp[::-1] \n",
    "    temp=np.flipud(temp) \n",
    "    return temp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "def plot_malicious_samples():\n",
    "    read_value=[96.00396728515625, 68.20671844482422, 96.05184936523438, 81.24053192138672, 107.14600372314453, 85.7899169921875, 100.9619140625, 181.16192626953125, 122.7833023071289, 119.53163146972656, 107.34815216064453, 108.99268341064453, 139.76922607421875, 97.21031951904297, 118.4515151977539, 99.15070343017578, 85.47505187988281, 81.51718139648438, 232.91493225097656, 82.83419799804688, 108.6827163696289, 95.99102020263672, 90.59868621826172, 85.60449981689453]\n",
    "    lists=[]\n",
    "    colors=['b','r-','g--','c:','m-.','y-','k--']\n",
    "    lists.append(read_value)\n",
    "    lists.append(h1(read_value))\n",
    "    lists.append(h2(read_value))\n",
    "    lists.append(h3(read_value))\n",
    "    lists.append(h4(read_value))\n",
    "    lists.append(h5(read_value))\n",
    "    lists.append(h6(read_value))\n",
    "    plt.figure(num=None, figsize=(14, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.title(\"malicious samples\")\n",
    "    plt.plot(read_value)\n",
    "    for i in range(len(lists)):\n",
    "        plt.plot(lists[i],colors[i],label = 'H %s'%i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "#plot_malicious_samples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Prepare Spark Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "def rename_dataframe(sdf):\n",
    "    names = ['#','date','id','power']\n",
    "    for c,n in zip(sdf.columns,names):\n",
    "        sdf=sdf.withColumnRenamed(c,n)\n",
    "    return sdf\n",
    "    \n",
    "#sdf=rename_dataframe(sdf)\n",
    "#sdf.show()\n",
    "    \n",
    "#convert power to array\n",
    "def string_power_to_array(sdf):\n",
    "    temp=sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"),\"\\\\]\",\"\")\n",
    "                        .alias(\"power\"))\n",
    "    temp=temp.withColumn(\"power\",split(col(\"power\"), \",\\s*\")\n",
    "                        .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "#sdf=string_power_to_array(sdf)\n",
    "#sdf.show()\n",
    "\n",
    "def add_validation_column(sdf):\n",
    "    def validation(x):\n",
    "        res=[]\n",
    "        for row in x:\n",
    "            v=True\n",
    "            if (len(row)!= 24 or # unusual size\n",
    "                (row>=0).sum()!=24 or #number of valid elements = 24\n",
    "                #sum(n >= 0 for n in row) != 24 or \n",
    "                np.count_nonzero(row == 0)>=3 or #equal or more than 3 zero elements\n",
    "                sum(n < 0 for n in row) > 0): #not have negative element\n",
    "                    v=False\n",
    "            res.append(v)\n",
    "        return pd.Series(res)\n",
    "    validation_UDF = pandas_udf(validation, returnType=BooleanType())\n",
    "    temp=sdf.withColumn(\"V\",validation_UDF(col(\"power\")))\n",
    "    return temp\n",
    "\n",
    "#sdf=add_validation_column(sdf)\n",
    "#sdf.show()\n",
    "\n",
    "\n",
    "#add \"N\"ormal consumption (\"N\"onmalicious) column\n",
    "def add_Normal_column(sdf):\n",
    "    N=True\n",
    "    temp=sdf.withColumn(\"N\", f.lit(N))\n",
    "    return temp\n",
    "\n",
    "#sdf=add_Normal_column(sdf)\n",
    "#sdf.show()\n",
    "\n",
    "#filter data\n",
    "def filter_dataset(sdf,from_date=\"BEGIN\",to_date=\"END\",ID=\"*\",V=\"*\"):\n",
    "    temp=sdf\n",
    "    if (from_date!=\"BEGIN\"):\n",
    "        temp=temp.filter(sdf.date > from_date) #filter date (from X)\n",
    "    if (to_date!=\"END\"):\n",
    "        temp=temp.filter(sdf.date < to_date) #filter date (to Y)\n",
    "    if (ID!=\"*\"):\n",
    "        temp=temp.filter(sdf.id == ID) #filter IDs\n",
    "    if (V!=\"*\"):\n",
    "        temp=temp.filter(sdf.V == V) #filter validation    \n",
    "    return temp\n",
    "\n",
    "#sdf=filter_dataset(sdf,from_date=\"BEGIN\",to_date=\"END\",ID=\"Apt36\",V=\"True\")\n",
    "#sdf.show()\n",
    "\n",
    "def split_power(sdf):\n",
    "    temp=sdf.select(\"#\",\"date\",\"id\",\n",
    "           sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\"H1\"), sdf.power[2].alias(\"H2\"),sdf.power[3].alias(\"H3\"),\n",
    "           sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\"H5\"), sdf.power[6].alias(\"H6\"),sdf.power[7].alias(\"H7\"),\n",
    "           sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\"H9\"), sdf.power[10].alias(\"H10\"),sdf.power[11].alias(\"H11\"),\n",
    "           sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\"H13\"), sdf.power[14].alias(\"H14\"),sdf.power[15].alias(\"H15\"),\n",
    "           sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\"H17\"), sdf.power[18].alias(\"H18\"),sdf.power[19].alias(\"H19\"),\n",
    "           sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"),sdf.power[23].alias(\"H23\"))\n",
    "    return temp\n",
    "    \n",
    "#split_sdf=split_power(sdf)\n",
    "#sdf.show()\n",
    "\n",
    "#make id list\n",
    "def get_ids(sdf):\n",
    "    IDs=np.array(sdf.select(\"id\").distinct().collect())\n",
    "    IDs=IDs.reshape(1,len(IDs))\n",
    "    return IDs\n",
    "\n",
    "\n",
    "#generate uniqe id\n",
    "def generate_uniqe_id(sdf):\n",
    "    temp=sdf\n",
    "    temp=temp.withColumn(\"uid\",f.concat(col(\"id\"),f.lit(\"-\"),col(\"#\")).alias(\"uid\"))\n",
    "    return temp\n",
    "\n",
    "#sdf=generate_uniqe_id(sdf)\n",
    "\n",
    "\n",
    "#sdf.show()\n",
    "#print(\"number of rows: \" + str(sdf.count()))\n",
    "#sdf.collect()\n",
    "#sdf.printSchema()\n",
    "#split_sdf=add_validation_column(split_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add malicious samples\n",
    "def create_malicious_df(sdf):\n",
    "    def random_attack_assigner(x):\n",
    "        NUMBER_OF_MALICIOUS_GENERATOR=6\n",
    "        res=[]\n",
    "        for row in x:\n",
    "            rand = random.randint(1,NUMBER_OF_MALICIOUS_GENERATOR)\n",
    "            if rand==1:\n",
    "                temp=(h1(row))\n",
    "            elif rand==2:\n",
    "                temp=(h2(row))\n",
    "            elif rand==3:\n",
    "                temp=(h3(row))\n",
    "            elif rand==4:\n",
    "                temp=(h4(row))\n",
    "            elif rand==5:\n",
    "                temp=(h5(row))\n",
    "            elif rand==6:\n",
    "                temp=(h6(row))\n",
    "            res.append(temp)\n",
    "        return pd.Series(res)\n",
    "    random_attack_assigner_UDF = pandas_udf(random_attack_assigner, returnType=ArrayType(FloatType()))\n",
    "    sdf_malicious=sdf\n",
    "    N=False\n",
    "    sdf_malicious=sdf_malicious.withColumn(\"N\", f.lit(N)) #malicious sample\n",
    "    sdf_malicious=sdf_malicious.withColumn(\"#\", col(\"#\")*-1) # change '#' column number to negative\n",
    "    sdf_malicious=sdf_malicious.withColumn(\"power\",random_attack_assigner_UDF(col(\"power\")))\n",
    "    return sdf_malicious\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_kmeans(sdf): \n",
    "    \n",
    "    temp=sdf\n",
    "    \n",
    "    #define function for split power column\n",
    "    def split_power(sdf):\n",
    "        temp=sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "           sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\"H1\"), sdf.power[2].alias(\"H2\"),sdf.power[3].alias(\"H3\"),\n",
    "           sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\"H5\"), sdf.power[6].alias(\"H6\"),sdf.power[7].alias(\"H7\"),\n",
    "           sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\"H9\"), sdf.power[10].alias(\"H10\"),sdf.power[11].alias(\"H11\"),\n",
    "           sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\"H13\"), sdf.power[14].alias(\"H14\"),sdf.power[15].alias(\"H15\"),\n",
    "           sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\"H17\"), sdf.power[18].alias(\"H18\"),sdf.power[19].alias(\"H19\"),\n",
    "           sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"),sdf.power[23].alias(\"H23\"))\n",
    "        return temp\n",
    "    \n",
    "    #call the split_power function\n",
    "    temp=split_power(temp)\n",
    "    \n",
    "    #filter date\n",
    "    #temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    #temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp=temp.filter(temp.V==True) #filter valid rows\n",
    "    \n",
    "    FEATURES = ['H0', 'H1', 'H2','H3', 'H4', 'H5','H6', 'H7', 'H8','H9', 'H10', 'H11',\n",
    "            'H12', 'H13', 'H14','H15', 'H16', 'H17','H18', 'H19', 'H20','H21', 'H22', 'H23']\n",
    "    \n",
    "    #call the generate_uniqe_id function\n",
    "    temp=generate_uniqe_id(temp)\n",
    "    \n",
    "    #make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_kmeans = vecAssembler.transform(temp).select(col(\"uid\"), col(\"features\"))\n",
    "    return df_kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run k-means\n",
    "\n",
    "def kmeans(sdf_kmeans):\n",
    "    #find best k\n",
    "    MAX_k=5\n",
    "    costs = np.zeros(MAX_k)\n",
    "    silhouettes = np.zeros(MAX_k)\n",
    "    silhouettes[1]=0 #set value for k=1\n",
    "    for k in range(2,MAX_k):\n",
    "        kmeans = KMeans().setK(k).setSeed(1)\n",
    "        model = kmeans.fit(sdf_kmeans)\n",
    "        costs[k] = model.computeCost(sdf_kmeans) # requires Spark 2.0 or later\n",
    "        predictions = model.transform(sdf_kmeans)\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        silhouettes[k] = evaluator.evaluate(predictions)\n",
    "\n",
    "#     #show silhouette\n",
    "#     fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "#     ax.plot(range(2,MAX_k),silhouettes[2:MAX_k])\n",
    "#     ax.set_xlabel('k')\n",
    "#     ax.set_ylabel('silhouette')\n",
    "\n",
    "#     #show cost\n",
    "#     fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "#     ax.plot(range(2,MAX_k),costs[2:MAX_k])\n",
    "#     ax.set_xlabel('k')\n",
    "#     ax.set_ylabel('cost')\n",
    "\n",
    "    # find best k\n",
    "    best_k = np.argmax(silhouettes)\n",
    "    print(\"maximum value of silhouette is: \" + str(silhouettes[best_k]) + \" in index: \" + str(best_k) )\n",
    "\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(best_k).setSeed(1)\n",
    "    model = kmeans.fit(sdf_kmeans)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(sdf_kmeans)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "#    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "#     # Shows the result.\n",
    "#     centers = model.clusterCenters()\n",
    "#     print(\"Cluster Centers: \")\n",
    "#     for center in centers:\n",
    "#         print(center)\n",
    "\n",
    "#     transformed = model.transform(sdf_kmeans).select('id', 'prediction')\n",
    "#     transformed.show()\n",
    "#     transformed.groupby('prediction').count().show()\n",
    "#     rows = transformed.collect() \n",
    "#     prediction = spark.createDataFrame(rows)\n",
    "#     prediction.show()\n",
    "    return model,best_k,silhouette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Decision Tree Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_decision_tree_methods(sdf): \n",
    "    \n",
    "    temp=sdf\n",
    "    \n",
    "    #define function for split power column\n",
    "    def split_power(sdf):\n",
    "        temp=sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "           sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\"H1\"), sdf.power[2].alias(\"H2\"),sdf.power[3].alias(\"H3\"),\n",
    "           sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\"H5\"), sdf.power[6].alias(\"H6\"),sdf.power[7].alias(\"H7\"),\n",
    "           sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\"H9\"), sdf.power[10].alias(\"H10\"),sdf.power[11].alias(\"H11\"),\n",
    "           sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\"H13\"), sdf.power[14].alias(\"H14\"),sdf.power[15].alias(\"H15\"),\n",
    "           sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\"H17\"), sdf.power[18].alias(\"H18\"),sdf.power[19].alias(\"H19\"),\n",
    "           sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"),sdf.power[23].alias(\"H23\"))\n",
    "        return temp\n",
    "    \n",
    "    #call the split_power function\n",
    "    temp=split_power(temp)\n",
    "    \n",
    "    #boolean to string (for \"N\" column)\n",
    "    temp=temp.withColumn(\"N\",f.col(\"N\").cast('string'))\n",
    "    #temp.printSchema()\n",
    "    \n",
    "    \n",
    "    #filter date\n",
    "    #temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    #temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp=temp.filter(temp.V==True) #filter valid rows\n",
    "    \n",
    "    FEATURES = ['H0', 'H1', 'H2','H3', 'H4', 'H5','H6', 'H7', 'H8','H9', 'H10', 'H11',\n",
    "            'H12', 'H13', 'H14','H15', 'H16', 'H17','H18', 'H19', 'H20','H21', 'H22', 'H23']\n",
    "    \n",
    "    #call the generate_uniqe_id function\n",
    "    #temp=generate_uniqe_id(temp)\n",
    "    \n",
    "    #make features ready\n",
    "    assembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    output = assembler.transform(temp)\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    \n",
    "    #make label ready\n",
    "    indexer = StringIndexer(inputCol=\"N\", outputCol=\"NIndex\")\n",
    "    output_fixed = indexer.fit(output).transform(output)\n",
    "    \n",
    "    final_data = output_fixed.select(\"features\",'NIndex')\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run decision tree methods\n",
    "def decision_tree(train_data,test_data):\n",
    "    dtc = DecisionTreeClassifier(labelCol='NIndex',featuresCol='features')\n",
    "    rfc = RandomForestClassifier(labelCol='NIndex',featuresCol='features') #,numTrees=100\n",
    "    gbt = GBTClassifier(labelCol='NIndex',featuresCol='features')\n",
    "\n",
    "    dtc_model = dtc.fit(train_data)\n",
    "    rfc_model = rfc.fit(train_data)\n",
    "    gbt_model = gbt.fit(train_data)\n",
    "\n",
    "    dtc_predictions = dtc_model.transform(test_data)\n",
    "    rfc_predictions = rfc_model.transform(test_data)\n",
    "    gbt_predictions = gbt_model.transform(test_data)\n",
    "    \n",
    "    #evaluation\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"NIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "    rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "    gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "    #print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "    #print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "    #print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n",
    "    \n",
    "    return dtc_acc,rfc_acc,gbt_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pca(sdf): \n",
    "    \n",
    "    temp=sdf\n",
    "    \n",
    "    #define function for split power column\n",
    "    def split_power(sdf):\n",
    "        temp=sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "           sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\"H1\"), sdf.power[2].alias(\"H2\"),sdf.power[3].alias(\"H3\"),\n",
    "           sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\"H5\"), sdf.power[6].alias(\"H6\"),sdf.power[7].alias(\"H7\"),\n",
    "           sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\"H9\"), sdf.power[10].alias(\"H10\"),sdf.power[11].alias(\"H11\"),\n",
    "           sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\"H13\"), sdf.power[14].alias(\"H14\"),sdf.power[15].alias(\"H15\"),\n",
    "           sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\"H17\"), sdf.power[18].alias(\"H18\"),sdf.power[19].alias(\"H19\"),\n",
    "           sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"),sdf.power[23].alias(\"H23\"))\n",
    "        return temp\n",
    "    \n",
    "    #call the split_power function\n",
    "    temp=split_power(temp)\n",
    "    \n",
    "    temp=temp.filter(temp.V==True) #filter valid rows\n",
    "    \n",
    "    FEATURES = ['H0', 'H1', 'H2','H3', 'H4', 'H5','H6', 'H7', 'H8','H9', 'H10', 'H11',\n",
    "            'H12', 'H13', 'H14','H15', 'H16', 'H17','H18', 'H19', 'H20','H21', 'H22', 'H23']\n",
    "\n",
    "    #call the generate_uniqe_id function\n",
    "    temp=generate_uniqe_id(temp)\n",
    "    \n",
    "    #make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_pca = vecAssembler.transform(temp).select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\", col(\"features\"))\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "def pca_for_tree(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\"NIndex\",col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "def pca_for_kmeans(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\"uid\",col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "#sdf_pca=pca(sdf)\n",
    "#sdf_pca.show() #truncate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"anomaly_detection\").master(\"local[20]\").getOrCreate()\n",
    "#network problem? type it in commandline: sudo hostname -s 127.0.0.1\n",
    "\n",
    "#define schema\n",
    "schema = StructType([\n",
    "    StructField(\"#\", IntegerType()),\n",
    "    StructField(\"date\", TimestampType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"power\", StringType())])\n",
    "\n",
    "#read data\n",
    "sdf = spark.read.format('csv').options(header='true',inferSchema=True ,schema=schema).load(\n",
    "    os.path.join(DATASET_PATH, 'aggregated_dataset_rowBased.csv'))\n",
    "\n",
    "\n",
    "sdf=rename_dataframe(sdf)\n",
    "sdf=string_power_to_array(sdf)\n",
    "sdf=add_validation_column(sdf)\n",
    "sdf=add_Normal_column(sdf)\n",
    "sdf=filter_dataset(sdf,from_date=\"BEGIN\",to_date=\"END\",ID=\"*\",V=\"True\")\n",
    "sdf=generate_uniqe_id(sdf)\n",
    "\n",
    "#ids\n",
    "id_list=get_ids(sdf)\n",
    "\n",
    "\n",
    "# #dataset\n",
    "# print(\"dataframe schema:\")\n",
    "# print(\"number of rows: \" + str(sdf.count()))\n",
    "# sdf.printSchema()\n",
    "# print(\"benign dataframe (sdf):\")\n",
    "# sdf.show()\n",
    "\n",
    "# #generate malicious data\n",
    "# sdf_malicious=create_malicious_df(sdf)\n",
    "# print(\"malicious dataframe (sdf_malicious):\")\n",
    "# sdf_malicious.show()\n",
    "# sdf_mix= sdf.union(sdf_malicious)\n",
    "# print(\"final dataframe (sdf_mix):\")\n",
    "# sdf_mix.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- k-means started!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>k</th>\n",
       "      <th>Silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apt107</td>\n",
       "      <td>2</td>\n",
       "      <td>0.787509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apt21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.784405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apt8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apt4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.870959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apt30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  k  Silhouette\n",
       "0  Apt107  2    0.787509\n",
       "1   Apt21  2    0.784405\n",
       "2    Apt8  2    0.786216\n",
       "3    Apt4  2    0.870959\n",
       "4   Apt30  2    0.834006"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kmeans separately\n",
    "\n",
    "def call_kmeans(sdf):\n",
    "    \n",
    "    #create statistics dataframe\n",
    "    kmeans_statistics_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"k\", IntegerType()),\n",
    "    StructField(\"Silhouette\", FloatType())])\n",
    "\n",
    "    kmeans_statistics = spark.createDataFrame([], kmeans_statistics_schema)    \n",
    "    \n",
    "    id_list=get_ids(sdf)\n",
    "    sdf_kmeans=prepare_for_kmeans(sdf) # replace sdf with final_sdf for clustring benign and malicious data\n",
    "    sdf_kmeans=pca_for_kmeans(sdf_kmeans) #0.8725788926917551 to 0.9101118371931005\n",
    "    #sdf_kmeans.show()\n",
    "    iteration=1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_kmeans_by_id = sdf_kmeans.filter(sdf_kmeans.uid.like(str(i)+\"-\"+\"%\")) #filter IDs\n",
    "        print(\"customer \"+ str(iteration)+\": \"+ str(i))\n",
    "        #sdf_kmeans_by_id.show()\n",
    "        kmeans_model,best_k,silhouette=kmeans(sdf_kmeans_by_id)\n",
    "        #kmeans_model.save(os.path.join(KMEANS_PATH,str(i)))\n",
    "        \n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), int(best_k), float(silhouette))])\n",
    "        kmeans_statistics = kmeans_statistics.union(newRow_for_statistics)\n",
    "        \n",
    "        iteration+=1\n",
    "        #model_name = KMeansModel.load(os.path.join(KMEANS_PATH,str(i)) #for load model\n",
    "    return kmeans_statistics\n",
    "\n",
    "print(\"-------------------- k-means started!\")\n",
    "#kmeans_statistics=call_kmeans(sdf)\n",
    "#kmeans_statistics.show()\n",
    "#save\n",
    "# result_pdf = kmeans_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "#load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- decision tree started!\n",
      "customer 1: Apt99\n",
      "A single decision tree had an accuracy of: 64.50%\n",
      "A random forest ensemble had an accuracy of: 69.70%\n",
      "A ensemble using GBT had an accuracy of: 72.94%\n",
      "+-----+-----------------+-----------------+------------------+\n",
      "|   id|              dtc|              rfc|               gbt|\n",
      "+-----+-----------------+-----------------+------------------+\n",
      "|Apt99|0.645021645021645|0.696969696969697|0.7294372294372294|\n",
      "+-----+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#decision tree separately\n",
    "def call_trees(sdf):\n",
    "    \n",
    "    #create statistics dataframe\n",
    "    trees_statistics_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"dtc\", FloatType()),\n",
    "    StructField(\"rfc\", FloatType()),\n",
    "    StructField(\"gbt\", FloatType())])\n",
    "\n",
    "    trees_statistics = spark.createDataFrame([], trees_statistics_schema)  \n",
    "    \n",
    "    id_list=get_ids(sdf)\n",
    "    \n",
    "    iteration=1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_trees_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\")) #filter IDs\n",
    "        print(\"customer \"+ str(iteration)+\": \"+ str(i))\n",
    "        \n",
    "        sdf_trees=prepare_for_decision_tree_methods(sdf)\n",
    "        train_data,test_data = sdf_trees.randomSplit([0.7,0.3])\n",
    "\n",
    "        dtc_acc,rfc_acc,gbt_acc=decision_tree(train_data,test_data)\n",
    "        print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "        print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "        print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n",
    "        \n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), float(dtc_acc), float(rfc_acc), float(gbt_acc))])\n",
    "        trees_statistics = trees_statistics.union(newRow_for_statistics)\n",
    "        \n",
    "        iteration+=1\n",
    "    return trees_statistics\n",
    "  \n",
    "#print(\"-------------------- decision tree started!\")\n",
    "#trees_statistics=call_trees(sdf_mix)\n",
    "#trees_statistics.show()\n",
    "#save\n",
    "# result_pdf = trees_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'trees_statistics.pkl'))\n",
    "#load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'trees_statistics.pkl'))\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>k</th>\n",
       "      <th>Silhouette</th>\n",
       "      <th>n_per_k</th>\n",
       "      <th>dtc</th>\n",
       "      <th>rfc</th>\n",
       "      <th>gbt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apt107</td>\n",
       "      <td>2</td>\n",
       "      <td>0.787509</td>\n",
       "      <td>[213, 567]</td>\n",
       "      <td>[0.7205882352941176, 0.6772334293948127]</td>\n",
       "      <td>[0.875, 0.7579250720461095]</td>\n",
       "      <td>[0.8602941176470589, 0.7521613832853026]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apt21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.784405</td>\n",
       "      <td>[532, 178]</td>\n",
       "      <td>[0.7272727272727273, 0.9247311827956989]</td>\n",
       "      <td>[0.7636363636363637, 0.8494623655913979]</td>\n",
       "      <td>[0.7454545454545455, 0.8494623655913979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apt8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786216</td>\n",
       "      <td>[245, 544]</td>\n",
       "      <td>[0.7346938775510204, 0.5714285714285714]</td>\n",
       "      <td>[0.8571428571428571, 0.6398809523809523]</td>\n",
       "      <td>[0.8503401360544217, 0.6845238095238095]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apt4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.870959</td>\n",
       "      <td>[661, 123]</td>\n",
       "      <td>[0.6326530612244898, 0.8421052631578947]</td>\n",
       "      <td>[0.7066326530612245, 0.868421052631579]</td>\n",
       "      <td>[0.6352040816326531, 0.868421052631579]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apt30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834006</td>\n",
       "      <td>[135, 637]</td>\n",
       "      <td>[0.7204301075268817, 0.7265822784810126]</td>\n",
       "      <td>[0.7741935483870968, 0.7417721518987341]</td>\n",
       "      <td>[0.8172043010752689, 0.7443037974683544]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  k  Silhouette     n_per_k  \\\n",
       "0  Apt107  2    0.787509  [213, 567]   \n",
       "1   Apt21  2    0.784405  [532, 178]   \n",
       "2    Apt8  2    0.786216  [245, 544]   \n",
       "3    Apt4  2    0.870959  [661, 123]   \n",
       "4   Apt30  2    0.834006  [135, 637]   \n",
       "\n",
       "                                        dtc  \\\n",
       "0  [0.7205882352941176, 0.6772334293948127]   \n",
       "1  [0.7272727272727273, 0.9247311827956989]   \n",
       "2  [0.7346938775510204, 0.5714285714285714]   \n",
       "3  [0.6326530612244898, 0.8421052631578947]   \n",
       "4  [0.7204301075268817, 0.7265822784810126]   \n",
       "\n",
       "                                        rfc  \\\n",
       "0               [0.875, 0.7579250720461095]   \n",
       "1  [0.7636363636363637, 0.8494623655913979]   \n",
       "2  [0.8571428571428571, 0.6398809523809523]   \n",
       "3   [0.7066326530612245, 0.868421052631579]   \n",
       "4  [0.7741935483870968, 0.7417721518987341]   \n",
       "\n",
       "                                        gbt  \n",
       "0  [0.8602941176470589, 0.7521613832853026]  \n",
       "1  [0.7454545454545455, 0.8494623655913979]  \n",
       "2  [0.8503401360544217, 0.6845238095238095]  \n",
       "3   [0.6352040816326531, 0.868421052631579]  \n",
       "4  [0.8172043010752689, 0.7443037974683544]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model\n",
    "def call_model(sdf):\n",
    "    id_list=get_ids(sdf)\n",
    "    \n",
    "    #create statistics dataframe\n",
    "    statistics_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"k\", IntegerType()),\n",
    "    StructField(\"Silhouette\", FloatType()),\n",
    "    StructField(\"n_per_k\", ArrayType(IntegerType())),\n",
    "    StructField(\"dtc\", ArrayType(FloatType())),\n",
    "    StructField(\"rfc\", ArrayType(FloatType())),\n",
    "    StructField(\"gbt\", ArrayType(FloatType()))])\n",
    "    \n",
    "    statistics = spark.createDataFrame([], statistics_schema)\n",
    "    \n",
    "    iteration=1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\")) #filter IDs\n",
    "        print(\"customer \"+ str(iteration)+\": \"+ str(i))\n",
    "        \n",
    "        sdf_kmeans=prepare_for_kmeans(sdf_by_id)\n",
    "        \n",
    "        #sdf_kmeans=pca_for_kmeans(sdf_kmeans) \n",
    "        \n",
    "        #train_data,test_data = sdf_kmeans.randomSplit([0.7,0.3])\n",
    "        kmeans_model,best_k,silhouette=kmeans(sdf_kmeans)\n",
    "        print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "        print(\"best k= \"+ str(best_k))\n",
    "        \n",
    "        transformed = kmeans_model.transform(sdf_kmeans).select('uid', 'prediction','features')\n",
    "        transformed.show()\n",
    "        sdf_join = transformed.join(sdf_by_id, on=['uid'], how='inner')\n",
    "        sdf_join.show()\n",
    "        \n",
    "        #define statistics variables\n",
    "        n_per_k=[]\n",
    "        dtc=[]\n",
    "        rfc=[]\n",
    "        gbt=[]\n",
    "        \n",
    "        for k in range(0,best_k):\n",
    "            temp_sdf=sdf_join.filter(sdf_join.prediction == k) \n",
    "            temp_sdf_malicious=create_malicious_df(temp_sdf)\n",
    "            temp_sdf_mixed= temp_sdf.union(temp_sdf_malicious)\n",
    "            tree_data=prepare_for_decision_tree_methods(temp_sdf_mixed)\n",
    "            \n",
    "            tree_data=pca_for_tree(tree_data)\n",
    "            \n",
    "            train_data,test_data = tree_data.randomSplit([0.7,0.3])\n",
    "            dtc_acc,rfc_acc,gbt_acc=decision_tree(train_data,test_data)\n",
    "            print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "            print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "            print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n",
    "            \n",
    "            n_per_k.append(int(temp_sdf.count()))\n",
    "            dtc.append(float(dtc_acc))\n",
    "            rfc.append(float(rfc_acc))\n",
    "            gbt.append(float(gbt_acc))\n",
    "            \n",
    "        \n",
    "        #update statistics\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), int(best_k), float(silhouette), n_per_k, dtc, rfc, gbt)])\n",
    "        statistics = statistics.union(newRow_for_statistics)\n",
    "        \n",
    "        iteration+=1\n",
    "        \n",
    "        \n",
    "    return statistics\n",
    "\n",
    "#statistics=call_model(sdf)\n",
    "#statistics.show()\n",
    "#save\n",
    "# result_pdf = statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "#load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Silhouette</th>\n",
       "      <th>dtc_acc</th>\n",
       "      <th>rfc_acc</th>\n",
       "      <th>gbt_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.026316</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.704515</td>\n",
       "      <td>0.756153</td>\n",
       "      <td>0.747912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.208684</td>\n",
       "      <td>0.067076</td>\n",
       "      <td>0.037462</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>0.035527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.622762</td>\n",
       "      <td>0.622126</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>0.657465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.749241</td>\n",
       "      <td>0.680039</td>\n",
       "      <td>0.731059</td>\n",
       "      <td>0.724466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.791309</td>\n",
       "      <td>0.705004</td>\n",
       "      <td>0.753453</td>\n",
       "      <td>0.744606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.836702</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.778757</td>\n",
       "      <td>0.769551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.939398</td>\n",
       "      <td>0.808726</td>\n",
       "      <td>0.859485</td>\n",
       "      <td>0.854846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                k  Silhouette     dtc_acc     rfc_acc     gbt_acc\n",
       "count  114.000000  114.000000  114.000000  114.000000  114.000000\n",
       "mean     2.026316    0.789654    0.704515    0.756153    0.747912\n",
       "std      0.208684    0.067076    0.037462    0.037286    0.035527\n",
       "min      2.000000    0.622762    0.622126    0.668492    0.657465\n",
       "25%      2.000000    0.749241    0.680039    0.731059    0.724466\n",
       "50%      2.000000    0.791309    0.705004    0.753453    0.744606\n",
       "75%      2.000000    0.836702    0.725209    0.778757    0.769551\n",
       "max      4.000000    0.939398    0.808726    0.859485    0.854846"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output of model\n",
    "df['dtc_acc'] = [np.dot(df.n_per_k.to_numpy()[i],df.dtc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "df['rfc_acc'] = [np.dot(df.n_per_k.to_numpy()[i],df.rfc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "df['gbt_acc'] = [np.dot(df.n_per_k.to_numpy()[i],df.gbt.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "df.head(20)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Useful Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.describe().show()\n",
    "#.printSchema()\n",
    "#.collect()\n",
    "#.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "# #create json data from stored dataframe\n",
    "\n",
    "# def to_json(final):\n",
    "#     PERIOD=60\n",
    "\n",
    "#     data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "#     def date_to_str(o):\n",
    "#         if isinstance(o, datetime.datetime):\n",
    "#             return o.__str__()\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "#     L  = []\n",
    "\n",
    "#     import json\n",
    "#     import datetime\n",
    "#     import time\n",
    "\n",
    "#     r, c = data_for_json.shape\n",
    "#     for i in range(0, r):\n",
    "#         for j in range(0, c):\n",
    "#             data = {}\n",
    "#             data['id'] = data_for_json.columns.values[j]\n",
    "#             data['power'] = data_for_json.iloc[i][j]\n",
    "#             data['date']=data_for_json.index.tolist()[i]\n",
    "#             json_data = json.dumps(data,default=date_to_str)\n",
    "#             L.append(json_data)\n",
    "#             #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "#     return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load by schema\n",
    "\n",
    "# schema = StructType([\n",
    "#   StructField(\"num\", IntegerType()),\n",
    "#     StructField(\"date\", TimestampType()),\n",
    "#     StructField(\"id\", StringType()),\n",
    "#   StructField(\"power\", ArrayType(\n",
    "#       StructType([\n",
    "#           StructField(\"H0\", FloatType(), True),\n",
    "#           StructField(\"H1\", FloatType(), True),\n",
    "#           StructField(\"H2\", FloatType(), True),\n",
    "#           StructField(\"H3\", FloatType(), True),\n",
    "#           StructField(\"H4\", FloatType(), True),\n",
    "#           StructField(\"H5\", FloatType(), True),\n",
    "#           StructField(\"H6\", FloatType(), True),\n",
    "#           StructField(\"H7\", FloatType(), True),\n",
    "#           StructField(\"H8\", FloatType(), True),\n",
    "#           StructField(\"H9\", FloatType(), True),\n",
    "#           StructField(\"H10\", FloatType(), True),\n",
    "#           StructField(\"H11\", FloatType(), True),\n",
    "#           StructField(\"H12\", FloatType(), True),\n",
    "#           StructField(\"H13\", FloatType(), True),\n",
    "#           StructField(\"H14\", FloatType(), True),\n",
    "#           StructField(\"H15\", FloatType(), True),\n",
    "#           StructField(\"H16\", FloatType(), True),\n",
    "#           StructField(\"H17\", FloatType(), True),\n",
    "#           StructField(\"H18\", FloatType(), True),\n",
    "#           StructField(\"H19\", FloatType(), True),\n",
    "#           StructField(\"H20\", FloatType(), True),\n",
    "#           StructField(\"H21\", FloatType(), True),\n",
    "#           StructField(\"H22\", FloatType(), True),\n",
    "#           StructField(\"H23\", FloatType(), True)\n",
    "#       ])\n",
    "#    )\n",
    "#              )])\n",
    "\n",
    "# a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "# a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf\n",
    "\n",
    "# my_schema = StructType([\n",
    "#     StructField(\"id\", IntegerType()),\n",
    "#     StructField(\"age\", IntegerType())])\n",
    "# df=spark.read.csv(\"test.csv\", header=True,schema=my_schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# def plus_one(a):\n",
    "#     return a+1\n",
    "\n",
    "# plus_one_udf = pandas_udf(plus_one, returnType=IntegerType())\n",
    "\n",
    "# df.select(plus_one_udf(col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf (for array input)\n",
    "\n",
    "# df = spark.createDataFrame([([1,2,3,4,5,6],'val1'),([4,5,6,7,8,9],'val2')],['col1','col2'])\n",
    "# df.show()\n",
    "\n",
    "# @pandas_udf(ArrayType(LongType()))\n",
    "# def func(v):\n",
    "#     res=[]\n",
    "#     for row in v:\n",
    "#         temp=[]\n",
    "#         for k in range(len(row)):\n",
    "#             if (k<=2) or (k>4):\n",
    "#                 temp.append(row[k])\n",
    "#             else:\n",
    "#                 temp.append(row[k]*0)\n",
    "#         res.append(temp)\n",
    "#     return pd.Series(res)\n",
    "\n",
    "# df.withColumn('col3',func(df.col1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
