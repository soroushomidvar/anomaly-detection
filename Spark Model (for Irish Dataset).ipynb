{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Imports and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=python3.6\n",
      "env: PYSPARK_PYTHON=python3.6\n",
      "env: ARROW_PRE_0_15_IPC_FORMAT=1\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, size, isnan, array_contains, when, count, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import pathlib\n",
    "import platform\n",
    "\n",
    "# env variables\n",
    "if platform.system() == 'Windows':\n",
    "    %env PYSPARK_DRIVER_PYTHON = python\n",
    "    %env PYSPARK_PYTHON = python\n",
    "elif platform.system() == 'Linux':\n",
    "    %env PYSPARK_DRIVER_PYTHON = python\n",
    "    %env PYSPARK_PYTHON = python3\n",
    "else:\n",
    "    %env PYSPARK_DRIVER_PYTHON = python3.6\n",
    "    %env PYSPARK_PYTHON = python3.6\n",
    "\n",
    "# incompatibility with Pyarrow\n",
    "# need to install Pyarrow 0.14.1 or lower or Set the environment variable ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "%env ARROW_PRE_0_15_IPC_FORMAT = 1\n",
    "\n",
    "# used versions:\n",
    "# spark='2.4.3' python='3.6' pyarrow='0.14.1'\n",
    "\n",
    "# for new system:\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# %pip install numpy\n",
    "# %pip install -U matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install Pyarrow==0.14.0\n",
    "# %env PYSPARK_DRIVER_PYTHON=python\n",
    "# %env PYSPARK_PYTHON=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Soroush/Desktop/Thesis/Code/Datasets/Irish/CER Electricity Revised March 2012\n",
      "/Users/Soroush/Desktop/Thesis/Code/Datasets/Irish/CER Electricity Revised March 2012/kmeans models\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "\n",
    "BASE_PATH = pathlib.Path().absolute()\n",
    "\n",
    "KMEANS_REL_PATH = os.path.join(os.path.join(os.path.join(\n",
    "    \"Datasets\", \"Irish\"), \"CER Electricity Revised March 2012\"), \"kmeans models\")\n",
    "DATASET_REL_PATH = os.path.join(\n",
    "    os.path.join(\"Datasets\", \"Irish\"), \"CER Electricity Revised March 2012\")\n",
    "\n",
    "\n",
    "DATASET_PATH = os.path.join(BASE_PATH, DATASET_REL_PATH)\n",
    "KMEANS_PATH = os.path.join(BASE_PATH, KMEANS_REL_PATH)\n",
    "\n",
    "print(DATASET_PATH)\n",
    "print(KMEANS_PATH)\n",
    "\n",
    "#from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "# load and save .read_pickle() and .to_pickle()\n",
    "\n",
    "# save\n",
    "# dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "# aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "# json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "# dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "# aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "# json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "# aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "# load\n",
    "#dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'dataset.pkl'))\n",
    "#aggregated_dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'aggregated_dataset.pkl'))\n",
    "\n",
    "#.pkl to .csv\n",
    "#dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'irish_aggregated_dataset.pkl'))\n",
    "#dataset.to_csv(os.path.join(DATASET_PATH, 'irish_aggregated_dataset.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **Malicious Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate malicious samples\n",
    "def h1(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    alpha = random.uniform(MIN, MAX)\n",
    "    temp = np.array(x)\n",
    "    return (temp*alpha).tolist()\n",
    "\n",
    "\n",
    "def h2(x):\n",
    "    MIN_OFF = 4  # hour\n",
    "    DURATION = random.randint(MIN_OFF, 23)\n",
    "    START = random.randint(0, 23-DURATION) if DURATION != 23 else 0\n",
    "    END = START+DURATION\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        if i < START or i >= END:\n",
    "            temp.append(x[i])\n",
    "        else:\n",
    "            temp.append(0.0)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h3(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(x[i]*random.uniform(MIN, MAX))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h4(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    mean = np.mean(x)\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean*random.uniform(MIN, MAX))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h5(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    mean = np.mean(x)\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h6(x):\n",
    "    temp = np.array(x)\n",
    "    # temp=temp[::-1]\n",
    "    temp = np.flipud(temp)\n",
    "    return temp.tolist()\n",
    "\n",
    "\n",
    "# add malicious samples\n",
    "def create_malicious_df(sdf):\n",
    "    def random_attack_assigner(x):\n",
    "        NUMBER_OF_MALICIOUS_GENERATOR = 6\n",
    "        res = []\n",
    "        for row in x:\n",
    "            rand = random.randint(1, NUMBER_OF_MALICIOUS_GENERATOR)\n",
    "            if rand == 1:\n",
    "                temp = (h1(row))\n",
    "            elif rand == 2:\n",
    "                temp = (h2(row))\n",
    "            elif rand == 3:\n",
    "                temp = (h3(row))\n",
    "            elif rand == 4:\n",
    "                temp = (h4(row))\n",
    "            elif rand == 5:\n",
    "                temp = (h5(row))\n",
    "            elif rand == 6:\n",
    "                temp = (h6(row))\n",
    "            res.append(temp)\n",
    "        return pd.Series(res)\n",
    "    random_attack_assigner_UDF = pandas_udf(\n",
    "        random_attack_assigner, returnType=ArrayType(FloatType()))\n",
    "    # sdf_malicious=sdf\n",
    "    N = False\n",
    "    sdf = sdf.withColumn(\"N\", f.lit(N))  # malicious sample\n",
    "    # change '#' column number to negative\n",
    "    sdf = sdf.withColumn(\"#\", col(\"#\")*-1)\n",
    "    sdf = sdf.withColumn(\"power\", random_attack_assigner_UDF(col(\"power\")))\n",
    "    # sdf=sdf.drop('statistics')\n",
    "    sdf = sdf.withColumn(\"statistics\", generate_feature_UDF(col(\"power\")))\n",
    "    sdf = add_statistics_column(sdf)  # for update statistics\n",
    "    return sdf.select(sdf.columns)  # to reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "def plot_malicious_samples():\n",
    "\n",
    "    read_value = [3.4803431034088135, 2.529871702194214, 2.2175486087799072, 2.629481077194214, 2.9629790782928467, 2.0697860717773438, 2.900712251663208, 2.926414966583252, 4.8191237449646, 4.156486988067627, 2.6474769115448, 2.1933677196502686,\n",
    "                  2.261159658432007, 2.340345621109009, 2.7386586666107178, 3.2414891719818115, 1.8946533203125, 3.1397650241851807, 2.8951449394226074, 3.4589333534240723, 2.726524829864502, 6.511429309844971, 3.4918391704559326, 3.787257432937622]\n",
    "    lists = []\n",
    "    colors = ['b', 'r-', 'g--', 'c:', 'm-.', 'y-', 'k--']\n",
    "    lists.append(read_value)\n",
    "    lists.append(h1(read_value))\n",
    "    lists.append(h2(read_value))\n",
    "    lists.append(h3(read_value))\n",
    "    lists.append(h4(read_value))\n",
    "    lists.append(h5(read_value))\n",
    "    lists.append(h6(read_value))\n",
    "    #font = {'size': 12}\n",
    "    #plt.rc('font', **font)\n",
    "    plt.figure(num=None, figsize=(14, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.xlabel(\"time (hour)\", fontsize=18)\n",
    "    plt.ylabel(\"usage (kw)\", fontsize=18)\n",
    "    #plt.title(\"malicious samples\")\n",
    "    plt.xticks(np.arange(0, 24, step=1))\n",
    "    plt.plot(read_value)\n",
    "    for i in range(len(lists)):\n",
    "        if i == 0:\n",
    "            plt.plot(lists[i], colors[i], label='normal usage')\n",
    "        else:\n",
    "            plt.plot(lists[i], colors[i], label='attack %s' % i)\n",
    "    # plt.legend()\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.savefig('attack.pdf', bbox_inches='tight')\n",
    "    # plt.savefig('attack.eps', format='eps')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# plot_malicious_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "## **Prepare Spark Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "def rename_dataframe(sdf):\n",
    "    names = ['#', 'id', 'date', 'power']\n",
    "    for c, n in zip(sdf.columns, names):\n",
    "        sdf = sdf.withColumnRenamed(c, n)\n",
    "    return sdf\n",
    "\n",
    "# sdf=rename_dataframe(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# convert power to array\n",
    "\n",
    "\n",
    "def string_power_to_array(sdf):\n",
    "    temp = sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"), \"\\\\]\", \"\")\n",
    "                          .alias(\"power\"))\n",
    "    temp = temp.withColumn(\"power\", split(col(\"power\"), \",\\s*\")\n",
    "                           .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "# sdf=string_power_to_array(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "def add_validation_column(sdf):\n",
    "    def validation(x):\n",
    "        res = []\n",
    "        for row in x:\n",
    "            v = True\n",
    "            if (len(row) != 24 or  # unusual size\n",
    "                (row >= 0).sum() != 24 or  # number of valid elements = 24\n",
    "                # sum(n >= 0 for n in row) != 24 or\n",
    "                # equal or more than 3 zero elements\n",
    "                np.count_nonzero(row == 0) >= 3 or\n",
    "                    sum(n < 0 for n in row) > 0):  # not have negative element\n",
    "                v = False\n",
    "            res.append(v)\n",
    "        return pd.Series(res)\n",
    "    validation_UDF = pandas_udf(validation, returnType=BooleanType())\n",
    "    temp = sdf.withColumn(\"V\", validation_UDF(col(\"power\")))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_validation_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "# add \"N\"ormal consumption (\"N\"onmalicious) column\n",
    "def add_Normal_column(sdf):\n",
    "    N = True\n",
    "    temp = sdf.withColumn(\"N\", f.lit(N))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_Normal_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# filter data\n",
    "\n",
    "\n",
    "def filter_dataset(sdf, from_date=\"BEGIN\", to_date=\"END\", ID=\"*\", V=\"*\"):\n",
    "    temp = sdf\n",
    "    if (from_date != \"BEGIN\"):\n",
    "        temp = temp.filter(sdf.date > from_date)  # filter date (from X)\n",
    "    if (to_date != \"END\"):\n",
    "        temp = temp.filter(sdf.date < to_date)  # filter date (to Y)\n",
    "    if (ID != \"*\"):\n",
    "        temp = temp.filter(sdf.id == ID)  # filter IDs\n",
    "    if (V != \"*\"):\n",
    "        temp = temp.filter(sdf.V == V)  # filter validation\n",
    "    return temp\n",
    "\n",
    "# sdf=filter_dataset(sdf,from_date=\"BEGIN\",to_date=\"END\",ID=\"Apt36\",V=\"True\")\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "def split_power(sdf):\n",
    "    temp = sdf.select(\"#\", \"date\", \"id\",\n",
    "                      sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                          \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                      sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                          \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                      sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                          \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                      sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                          \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                      sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                          \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                      sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"))\n",
    "    return temp\n",
    "\n",
    "# split_sdf=split_power(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# make id list\n",
    "\n",
    "\n",
    "def get_ids(sdf):\n",
    "    IDs = np.array(sdf.select(\"id\").distinct().collect())\n",
    "    IDs = IDs.reshape(1, len(IDs))\n",
    "    return IDs\n",
    "\n",
    "\n",
    "# generate uniqe id\n",
    "def generate_uniqe_id(sdf):\n",
    "    temp = sdf\n",
    "    temp = temp.withColumn(\"uid\", f.concat(\n",
    "        col(\"id\"), f.lit(\"-\"), col(\"#\")).alias(\"uid\"))\n",
    "    return temp\n",
    "\n",
    "# sdf=generate_uniqe_id(sdf)\n",
    "\n",
    "\n",
    "# generate feature\n",
    "def generate_feature(x):\n",
    "    res = []\n",
    "    for row in x:\n",
    "        row = np.array(row)  # to numpy\n",
    "        statistics = []\n",
    "        min_val = np.nanmin(row)\n",
    "        max_val = np.nanmax(row)\n",
    "        mean_val = np.nanmean(row)\n",
    "        std_val = np.nanstd(row)\n",
    "        statistics.append(mean_val)\n",
    "        statistics.append(std_val)\n",
    "        statistics.append(min_val)\n",
    "        statistics.append(max_val)\n",
    "        res.append(statistics)\n",
    "    return pd.Series(res)\n",
    "\n",
    "\n",
    "generate_feature_UDF = pandas_udf(\n",
    "    generate_feature, returnType=ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "def add_statistics_column(sdf):\n",
    "    temp = sdf.withColumn(\"statistics\", generate_feature_UDF(col(\"power\")))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_statistics_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "# sdf.show()\n",
    "# print(\"number of rows: \" + str(sdf.count()))\n",
    "# sdf.collect()\n",
    "# sdf.printSchema()\n",
    "# split_sdf=add_validation_column(split_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "## **K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_kmeans(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    temp = generate_uniqe_id(temp)\n",
    "\n",
    "    # make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_kmeans = vecAssembler.transform(\n",
    "        temp).select(col(\"uid\"), col(\"features\"))\n",
    "    return df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means\n",
    "\n",
    "\n",
    "def kmeans(sdf_kmeans):\n",
    "    # find best k\n",
    "    MAX_k = 8\n",
    "    costs = np.zeros(MAX_k)\n",
    "    silhouettes = np.zeros(MAX_k)\n",
    "    silhouettes[1] = 0  # set value for k=1\n",
    "    for k in range(2, MAX_k):\n",
    "        kmeans = KMeans().setK(k).setSeed(1)\n",
    "        model = kmeans.fit(sdf_kmeans)\n",
    "        costs[k] = model.computeCost(sdf_kmeans)  # requires Spark 2.0 or later\n",
    "        predictions = model.transform(sdf_kmeans)\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        silhouettes[k] = evaluator.evaluate(predictions)\n",
    "\n",
    "    # show silhouette\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(2, MAX_k), silhouettes[2:MAX_k])\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('silhouette')\n",
    "\n",
    "    # show cost\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(2, MAX_k), costs[2:MAX_k])\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('cost')\n",
    "\n",
    "    # find best k\n",
    "    best_k = np.argmax(silhouettes)\n",
    "    print(\"maximum value of silhouette is: \" +\n",
    "          str(silhouettes[best_k]) + \" in index: \" + str(best_k))\n",
    "\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(best_k).setSeed(1)\n",
    "    model = kmeans.fit(sdf_kmeans)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(sdf_kmeans)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "#     print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "#     # Shows the result.\n",
    "#     centers = model.clusterCenters()\n",
    "#     print(\"Cluster Centers: \")\n",
    "#     for center in centers:\n",
    "#         print(center)\n",
    "\n",
    "#     transformed = model.transform(sdf_kmeans).select('id', 'prediction')\n",
    "#     transformed.show()\n",
    "#     transformed.groupby('prediction').count().show()\n",
    "#     rows = transformed.collect()\n",
    "#     prediction = spark.createDataFrame(rows)\n",
    "#     prediction.show()\n",
    "    return model, best_k, silhouette  # silhouettes: new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **Decision Tree Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_decision_tree_methods(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # boolean to string (for \"N\" column)\n",
    "    temp = temp.withColumn(\"N\", f.col(\"N\").cast('string'))\n",
    "    # temp.printSchema()\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    # temp=generate_uniqe_id(temp)\n",
    "\n",
    "    # make features ready\n",
    "    assembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    output = assembler.transform(temp)\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "\n",
    "    # make label ready\n",
    "    indexer = StringIndexer(inputCol=\"N\", outputCol=\"NIndex\")\n",
    "    output_fixed = indexer.fit(output).transform(output)\n",
    "\n",
    "    final_data = output_fixed.select(\"features\", 'NIndex')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run decision tree methods\n",
    "def decision_tree(train_data, test_data):\n",
    "    dtc = DecisionTreeClassifier(labelCol='NIndex', featuresCol='features')\n",
    "    rfc = RandomForestClassifier(\n",
    "        labelCol='NIndex', featuresCol='features')  # ,numTrees=100\n",
    "    gbt = GBTClassifier(labelCol='NIndex', featuresCol='features')\n",
    "\n",
    "    dtc_model = dtc.fit(train_data)\n",
    "    rfc_model = rfc.fit(train_data)\n",
    "    gbt_model = gbt.fit(train_data)\n",
    "\n",
    "    dtc_predictions = dtc_model.transform(test_data)\n",
    "    rfc_predictions = rfc_model.transform(test_data)\n",
    "    gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "    # evaluation\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"NIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "    rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "    gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "    # new:\n",
    "    # Let's use the run-of-the-mill evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='NIndex')\n",
    "    # We have only two choices: area under ROC and PR curves :-(\n",
    "    dtc_auroc = evaluator.evaluate(\n",
    "        dtc_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    dtc_auprc = evaluator.evaluate(\n",
    "        dtc_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    rfc_auroc = evaluator.evaluate(\n",
    "        rfc_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    rfc_auprc = evaluator.evaluate(\n",
    "        rfc_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    gbt_auroc = evaluator.evaluate(\n",
    "        gbt_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    gbt_auprc = evaluator.evaluate(\n",
    "        gbt_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    #print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "    #print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "    #print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "    #print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "    #print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "    #print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "    #print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "    #print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "    #print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n",
    "\n",
    "    return dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pca(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split_power(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split_power(temp)\n",
    "\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    temp = generate_uniqe_id(temp)\n",
    "\n",
    "    # make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_pca = vecAssembler.transform(temp).select(\n",
    "        \"#\", \"V\", \"N\", \"date\", \"id\", \"uid\", col(\"features\"))\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "def pca_for_tree(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\n",
    "        \"NIndex\", col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "def pca_for_kmeans(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\n",
    "        \"uid\", col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "# sdf_pca=pca(sdf)\n",
    "# sdf_pca.show() #truncate=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlp(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # boolean to string (for \"N\" column)\n",
    "    temp = temp.withColumn(\"N\", f.col(\"N\").cast('string'))\n",
    "    # temp.printSchema()\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    # temp=generate_uniqe_id(temp)\n",
    "\n",
    "    # make features ready\n",
    "    assembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    output = assembler.transform(temp)\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "\n",
    "    # make label ready\n",
    "    indexer = StringIndexer(inputCol=\"N\", outputCol=\"label\")\n",
    "    output_fixed = indexer.fit(output).transform(output)\n",
    "\n",
    "    final_data = output_fixed.select(\"features\", 'label')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run mlp method\n",
    "def mlp(train_data, test_data, layers=[28, 50, 10, 2]):\n",
    "    # specify layers for the neural network:\n",
    "    # input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "    # and output of size 3 (classes)\n",
    "\n",
    "    # create the trainer and set its parameters\n",
    "    trainer = MultilayerPerceptronClassifier(\n",
    "        maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "    # train_data.show()\n",
    "    # train the model\n",
    "    model = trainer.fit(train_data)\n",
    "\n",
    "    # compute accuracy on the test set\n",
    "    result = model.transform(test_data)\n",
    "    predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        metricName=\"accuracy\")  # BinaryClassificationEvaluator\n",
    "    acc = evaluator.evaluate(predictionAndLabels)\n",
    "    print(\"Test set accuracy = \" + str(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign dataframe (sdf):\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "|  #|  id|               date|               power|          statistics|   V|   N|\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "|  0|1000|2009-07-14 00:00:00|[0.117, 0.0375, 0...|[0.23266667, 0.48...|true|true|\n",
      "|  1|1000|2009-07-15 00:00:00|[0.1075, 0.024, 0...|[0.17541666, 0.28...|true|true|\n",
      "|  2|1000|2009-07-16 00:00:00|[0.148, 0.0405, 0...|[0.14514583, 0.11...|true|true|\n",
      "|  3|1000|2009-07-17 00:00:00|[0.309, 0.0615, 0...|[0.24127083, 0.55...|true|true|\n",
      "|  4|1000|2009-07-18 00:00:00|[0.1585, 0.0425, ...|[0.23689584, 0.47...|true|true|\n",
      "|  5|1000|2009-07-19 00:00:00|[0.085, 0.0245, 0...|[0.06041667, 0.03...|true|true|\n",
      "|  6|1000|2009-07-20 00:00:00|[0.0425, 0.0715, ...|[0.24058335, 0.45...|true|true|\n",
      "|  7|1000|2009-07-21 00:00:00|[0.08, 0.028, 0.0...|[0.09670833, 0.06...|true|true|\n",
      "|  8|1000|2009-07-22 00:00:00|[0.052, 0.0515, 0...|[0.23891668, 0.46...|true|true|\n",
      "|  9|1000|2009-07-23 00:00:00|[1.1205, 0.257, 0...|[0.12427082, 0.22...|true|true|\n",
      "| 10|1000|2009-07-24 00:00:00|[0.0665, 0.033, 0...|[0.0711875, 0.053...|true|true|\n",
      "| 11|1000|2009-07-25 00:00:00|[0.259, 0.0875, 0...|[0.23697917, 0.45...|true|true|\n",
      "| 12|1000|2009-07-26 00:00:00|[0.063, 0.0585, 0...|[0.3695, 0.658563...|true|true|\n",
      "| 13|1000|2009-07-27 00:00:00|[0.0695, 0.0765, ...|[0.23104167, 0.47...|true|true|\n",
      "| 14|1000|2009-07-28 00:00:00|[0.1095, 0.0315, ...|[0.21902083, 0.48...|true|true|\n",
      "| 15|1000|2009-07-29 00:00:00|[0.0685, 0.1575, ...|[0.20574999, 0.41...|true|true|\n",
      "| 16|1000|2009-07-30 00:00:00|[0.0935, 0.025, 0...|[0.20602083, 0.53...|true|true|\n",
      "| 17|1000|2009-07-31 00:00:00|[0.1555, 0.058, 0...|[0.36395833, 0.46...|true|true|\n",
      "| 18|1000|2009-08-01 00:00:00|[0.275, 0.1015, 0...|[0.19185416, 0.44...|true|true|\n",
      "| 19|1000|2009-08-02 00:00:00|[0.0635, 0.025, 0...|[0.05172917, 0.02...|true|true|\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "malicious dataframe (sdf_malicious):\n",
      "+---+----+-------------------+--------------------+--------------------+----+-----+\n",
      "|  #|  id|               date|               power|          statistics|   V|    N|\n",
      "+---+----+-------------------+--------------------+--------------------+----+-----+\n",
      "|  0|1000|2009-07-14 00:00:00|[0.11108508, 0.11...|[0.093196206, 0.0...|true|false|\n",
      "| -1|1000|2009-07-15 00:00:00|[0.027965559, 0.0...|[0.092840485, 0.0...|true|false|\n",
      "| -2|1000|2009-07-16 00:00:00|[0.079, 0.023, 0....|[0.14514583, 0.11...|true|false|\n",
      "| -3|1000|2009-07-17 00:00:00|[0.24127083, 0.24...|[0.2085625, 0.558...|true|false|\n",
      "| -4|1000|2009-07-18 00:00:00|[0.076121844, 0.0...|[0.085206784, 0.1...|true|false|\n",
      "| -5|1000|2009-07-19 00:00:00|[0.1835, 0.023, 0...|[0.026177265, 0.0...|true|false|\n",
      "| -6|1000|2009-07-20 00:00:00|[0.043793242, 0.1...|[0.13172917, 0.41...|true|false|\n",
      "| -7|1000|2009-07-21 00:00:00|[0.051857762, 0.0...|[0.09670833, 0.0,...|true|false|\n",
      "| -8|1000|2009-07-22 00:00:00|[0.028668564, 0.0...|[0.13293086, 0.27...|true|false|\n",
      "| -9|1000|2009-07-23 00:00:00|[1.1205, 0.257, 0...|[0.02491662, 0.04...|true|false|\n",
      "|-10|1000|2009-07-24 00:00:00|[0.265, 0.109, 0....|[0.031655554, 0.0...|true|false|\n",
      "|-11|1000|2009-07-25 00:00:00|[0.259, 0.0875, 0...|[0.23697916, 0.45...|true|false|\n",
      "|-12|1000|2009-07-26 00:00:00|[0.0965, 0.079, 0...|[0.36949998, 0.65...|true|false|\n",
      "|-13|1000|2009-07-27 00:00:00|[0.009812894, 0.0...|[0.07510317, 0.15...|true|false|\n",
      "|-14|1000|2009-07-28 00:00:00|[0.16812964, 0.16...|[0.21902083, 0.48...|true|false|\n",
      "|-15|1000|2009-07-29 00:00:00|[0.621, 2.0825, 0...|[0.10960123, 0.22...|true|false|\n",
      "|-16|1000|2009-07-30 00:00:00|[0.07205155, 0.01...|[0.065867215, 0.1...|true|false|\n",
      "|-17|1000|2009-07-31 00:00:00|[0.415, 0.2865, 0...|[0.20297916, 0.33...|true|false|\n",
      "|-18|1000|2009-08-01 00:00:00|[0.0505, 0.036, 0...|[0.1401808, 0.324...|true|false|\n",
      "|-19|1000|2009-08-02 00:00:00|[0.030535575, 0.0...|[0.02081012, 0.01...|true|false|\n",
      "+---+----+-------------------+--------------------+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "final dataframe (sdf_mix):\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "|  #|  id|               date|               power|          statistics|   V|   N|\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "|  0|1000|2009-07-14 00:00:00|[0.117, 0.0375, 0...|[0.23266667, 0.48...|true|true|\n",
      "|  1|1000|2009-07-15 00:00:00|[0.1075, 0.024, 0...|[0.17541666, 0.28...|true|true|\n",
      "|  2|1000|2009-07-16 00:00:00|[0.148, 0.0405, 0...|[0.14514583, 0.11...|true|true|\n",
      "|  3|1000|2009-07-17 00:00:00|[0.309, 0.0615, 0...|[0.24127083, 0.55...|true|true|\n",
      "|  4|1000|2009-07-18 00:00:00|[0.1585, 0.0425, ...|[0.23689584, 0.47...|true|true|\n",
      "|  5|1000|2009-07-19 00:00:00|[0.085, 0.0245, 0...|[0.06041667, 0.03...|true|true|\n",
      "|  6|1000|2009-07-20 00:00:00|[0.0425, 0.0715, ...|[0.24058335, 0.45...|true|true|\n",
      "|  7|1000|2009-07-21 00:00:00|[0.08, 0.028, 0.0...|[0.09670833, 0.06...|true|true|\n",
      "|  8|1000|2009-07-22 00:00:00|[0.052, 0.0515, 0...|[0.23891668, 0.46...|true|true|\n",
      "|  9|1000|2009-07-23 00:00:00|[1.1205, 0.257, 0...|[0.12427082, 0.22...|true|true|\n",
      "| 10|1000|2009-07-24 00:00:00|[0.0665, 0.033, 0...|[0.0711875, 0.053...|true|true|\n",
      "| 11|1000|2009-07-25 00:00:00|[0.259, 0.0875, 0...|[0.23697917, 0.45...|true|true|\n",
      "| 12|1000|2009-07-26 00:00:00|[0.063, 0.0585, 0...|[0.3695, 0.658563...|true|true|\n",
      "| 13|1000|2009-07-27 00:00:00|[0.0695, 0.0765, ...|[0.23104167, 0.47...|true|true|\n",
      "| 14|1000|2009-07-28 00:00:00|[0.1095, 0.0315, ...|[0.21902083, 0.48...|true|true|\n",
      "| 15|1000|2009-07-29 00:00:00|[0.0685, 0.1575, ...|[0.20574999, 0.41...|true|true|\n",
      "| 16|1000|2009-07-30 00:00:00|[0.0935, 0.025, 0...|[0.20602083, 0.53...|true|true|\n",
      "| 17|1000|2009-07-31 00:00:00|[0.1555, 0.058, 0...|[0.36395833, 0.46...|true|true|\n",
      "| 18|1000|2009-08-01 00:00:00|[0.275, 0.1015, 0...|[0.19185416, 0.44...|true|true|\n",
      "| 19|1000|2009-08-02 00:00:00|[0.0635, 0.025, 0...|[0.05172917, 0.02...|true|true|\n",
      "+---+----+-------------------+--------------------+--------------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"anomaly_detection\").master(\"local[20]\").getOrCreate()\n",
    "# network problem? type it in commandline: sudo hostname -s 127.0.0.1\n",
    "\n",
    "# define schema\n",
    "schema = StructType([\n",
    "    StructField(\"#\", IntegerType()),\n",
    "    StructField(\"id\", TimestampType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"power\", StringType())])\n",
    "\n",
    "# read data\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema=True, schema=schema).load(\n",
    "    os.path.join(DATASET_PATH, 'irish_aggregated_dataset.csv'))\n",
    "\n",
    "\n",
    "sdf = rename_dataframe(sdf)\n",
    "sdf = string_power_to_array(sdf)\n",
    "sdf = add_statistics_column(sdf)\n",
    "sdf = add_validation_column(sdf)\n",
    "sdf = add_Normal_column(sdf)\n",
    "sdf = filter_dataset(sdf, from_date=\"BEGIN\",\n",
    "                     to_date=\"END\", ID=\"1000\", V=\"True\")\n",
    "sdf = generate_uniqe_id(sdf)\n",
    "\n",
    "# ids\n",
    "#id_list = get_ids(sdf)\n",
    "\n",
    "# #dataset\n",
    "# print(\"dataframe schema:\")\n",
    "# print(\"number of rows: \" + str(sdf.count()))\n",
    "# sdf.printSchema()\n",
    "print(\"benign dataframe (sdf):\")\n",
    "sdf.show()\n",
    "\n",
    "# #generate malicious data\n",
    "sdf_malicious = create_malicious_df(sdf)\n",
    "print(\"malicious dataframe (sdf_malicious):\")\n",
    "sdf_malicious.show()\n",
    "\n",
    "sdf_mix = sdf.union(sdf_malicious)\n",
    "print(\"final dataframe (sdf_mix):\")\n",
    "sdf_mix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a row\n",
    "# sdf.collect()[1][3] #[8027][3] is used for figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans separately\n",
    "\n",
    "\n",
    "def call_kmeans(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    kmeans_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType())])\n",
    "\n",
    "    kmeans_statistics = spark.createDataFrame([], kmeans_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "    # replace sdf with final_sdf for clustring benign and malicious data\n",
    "    sdf_kmeans = prepare_for_kmeans(sdf)\n",
    "    # sdf_kmeans=pca_for_kmeans(sdf_kmeans) #0.8725788926917551 to 0.9101118371931005\n",
    "    # sdf_kmeans.show()\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_kmeans_by_id = sdf_kmeans.filter(\n",
    "            sdf_kmeans.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "        # sdf_kmeans_by_id.show()\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans_by_id)\n",
    "        # kmeans_model.save(os.path.join(KMEANS_PATH,str(i)))\n",
    "        summary = kmeans_model.summary\n",
    "        if summary.clusterSizes[1] > 200:\n",
    "            print(\"AAAAAAAAAA\")\n",
    "        else:\n",
    "            print(\"BBBBBBBBBB\")\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame(\n",
    "            [(str(i), int(best_k), float(silhouette))])\n",
    "        kmeans_statistics = kmeans_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "        # model_name = KMeansModel.load(os.path.join(KMEANS_PATH,str(i)) #for load model\n",
    "    return kmeans_statistics\n",
    "\n",
    "\n",
    "print(\"-------------------- k-means started!\")\n",
    "kmeans_statistics = call_kmeans(sdf)\n",
    "kmeans_statistics.show()\n",
    "# save\n",
    "# result_pdf = kmeans_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "# df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree separately\n",
    "def call_trees(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    trees_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"dtc_acc\", FloatType()),\n",
    "        StructField(\"dtc_auroc\", FloatType()),\n",
    "        StructField(\"dtc_auprc\", FloatType()),\n",
    "        StructField(\"rfc_acc\", FloatType()),\n",
    "        StructField(\"rfc_auroc\", FloatType()),\n",
    "        StructField(\"rfc_auprc\", FloatType()),\n",
    "        StructField(\"gbt_acc\", FloatType()),\n",
    "        StructField(\"gbt_auroc\", FloatType()),\n",
    "        StructField(\"gbt_auprc\", FloatType())])\n",
    "\n",
    "    trees_statistics = spark.createDataFrame([], trees_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_trees_by_id = sdf.filter(\n",
    "            sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_trees_by_id_malicious = create_malicious_df(sdf_trees_by_id)\n",
    "        sdf_trees_by_id_mixed = sdf_trees_by_id.union(\n",
    "            sdf_trees_by_id_malicious)\n",
    "\n",
    "        # sdf_trees=prepare_for_decision_tree_methods(sdf)\n",
    "\n",
    "        sdf_trees = prepare_for_decision_tree_methods(sdf_trees_by_id_mixed)\n",
    "        train_data, test_data = sdf_trees.randomSplit([0.7, 0.3])\n",
    "\n",
    "        dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc = decision_tree(\n",
    "            train_data, test_data)\n",
    "\n",
    "        print('A single decision tree had an accuracy of: {0:2.2f}%'.format(\n",
    "            dtc_acc*100))\n",
    "        print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "        print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "        print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(\n",
    "            rfc_acc*100))\n",
    "        print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "        print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "        print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(\n",
    "            gbt_acc*100))\n",
    "        print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "        print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), float(dtc_acc), float(dtc_auroc), float(dtc_auprc),\n",
    "                                                        float(rfc_acc), float(\n",
    "                                                            rfc_auroc), float(rfc_auprc),\n",
    "                                                        float(gbt_acc), float(gbt_auroc), float(gbt_auprc))])\n",
    "        trees_statistics = trees_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "    return trees_statistics\n",
    "\n",
    "\n",
    "#print(\"-------------------- decision tree started!\")\n",
    "# trees_statistics=call_trees(sdf)\n",
    "# trees_statistics.show()\n",
    "# save\n",
    "# result_pdf = trees_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'trees_statistics.pkl'))\n",
    "# load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'all_statistics_trees_k1.pkl'))\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp separetely\n",
    "def call_mlp(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    mlp_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"mlp\", FloatType())])\n",
    "\n",
    "    mlp_statistics = spark.createDataFrame([], mlp_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_trees_by_id = sdf.filter(\n",
    "            sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_mlp = prepare_for_mlp(sdf)\n",
    "        train_data, test_data = sdf_mlp.randomSplit([0.7, 0.3])\n",
    "\n",
    "        acc = mlp(train_data, test_data, [28, 60, 10, 2])\n",
    "\n",
    "        print('A MLP had an accuracy of: {0:2.2f}%'.format(acc*100))\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), float(acc))])\n",
    "        mlp_statistics = mlp_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "    return mlp_statistics\n",
    "\n",
    "#print(\"-------------------- mlp started!\")\n",
    "# run\n",
    "# mlp_statistics=call_mlp(sdf_mix)\n",
    "# mlp_statistics.show()\n",
    "\n",
    "# save\n",
    "#df = statistics.select(\"*\").toPandas()\n",
    "#df.to_pickle(os.path.join(BASE_PATH, 'mlp_statistics.pkl'))\n",
    "\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'mlp_statistics.pkl'))\n",
    "# df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model tree\n",
    "def call_model_with_tree(sdf):\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    # create statistics dataframe\n",
    "    statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType()),\n",
    "        StructField(\"n_per_k\", ArrayType(IntegerType())),\n",
    "        StructField(\"dtc_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"dtc_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"dtc_auprc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_auprc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_auprc\", ArrayType(FloatType()))])\n",
    "\n",
    "    statistics = spark.createDataFrame([], statistics_schema)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_kmeans = prepare_for_kmeans(sdf_by_id)\n",
    "\n",
    "        # sdf_kmeans=pca_for_kmeans(sdf_kmeans)\n",
    "\n",
    "        #train_data,test_data = sdf_kmeans.randomSplit([0.7,0.3])\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans)\n",
    "        print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "        print(\"best k= \" + str(best_k))\n",
    "\n",
    "        transformed = kmeans_model.transform(\n",
    "            sdf_kmeans).select('uid', 'prediction', 'features')\n",
    "        # transformed.show()\n",
    "        sdf_join = transformed.join(sdf_by_id, on=['uid'], how='inner')\n",
    "        # sdf_join.show()\n",
    "\n",
    "        # define statistics variables\n",
    "        n_per_k = []\n",
    "        dtc_acc_list = []\n",
    "        dtc_auroc_list = []\n",
    "        dtc_auprc_list = []\n",
    "        rfc_acc_list = []\n",
    "        rfc_auroc_list = []\n",
    "        rfc_auprc_list = []\n",
    "        gbt_acc_list = []\n",
    "        gbt_auroc_list = []\n",
    "        gbt_auprc_list = []\n",
    "\n",
    "        for k in range(0, best_k):\n",
    "            temp_sdf = sdf_join.filter(sdf_join.prediction == k)\n",
    "            temp_sdf_malicious = create_malicious_df(temp_sdf)\n",
    "            temp_sdf_mixed = temp_sdf.union(temp_sdf_malicious)\n",
    "            tree_data = prepare_for_decision_tree_methods(temp_sdf_mixed)\n",
    "\n",
    "            # tree_data=pca_for_tree(tree_data)\n",
    "\n",
    "            train_data, test_data = tree_data.randomSplit([0.7, 0.3])\n",
    "            dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc = decision_tree(\n",
    "                train_data, test_data)\n",
    "            print('A single decision tree had an accuracy of: {0:2.2f}%'.format(\n",
    "                dtc_acc*100))\n",
    "            print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "            print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "            print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(\n",
    "                rfc_acc*100))\n",
    "            print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "            print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "            print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(\n",
    "                gbt_acc*100))\n",
    "            print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "            print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "            n_per_k.append(int(temp_sdf.count()))\n",
    "            dtc_acc_list.append(float(dtc_acc))\n",
    "            dtc_auroc_list.append(float(dtc_auroc))\n",
    "            dtc_auprc_list.append(float(dtc_auprc))\n",
    "            rfc_acc_list.append(float(rfc_acc))\n",
    "            rfc_auroc_list.append(float(rfc_auroc))\n",
    "            rfc_auprc_list.append(float(rfc_auprc))\n",
    "            gbt_acc_list.append(float(gbt_acc))\n",
    "            gbt_auroc_list.append(float(gbt_auroc))\n",
    "            gbt_auprc_list.append(float(gbt_auprc))\n",
    "\n",
    "        # update statistics\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), int(best_k), float(silhouette), n_per_k,\n",
    "                                                        dtc_acc_list, dtc_auroc_list, dtc_auprc_list,\n",
    "                                                        rfc_acc_list, rfc_auroc_list, rfc_auprc_list,\n",
    "                                                        gbt_acc_list, gbt_auroc_list, gbt_auprc_list)])\n",
    "        statistics = statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return statistics\n",
    "\n",
    "# statistics=call_model_with_tree(sdf)\n",
    "# statistics.show()\n",
    "\n",
    "# save\n",
    "# result_pdf = statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "\n",
    "\n",
    "# load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'all_statistics_trees_k2to5.pkl'))\n",
    "df.head()\n",
    "\n",
    "# output of model\n",
    "#df['dtc_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.dtc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "#df['rfc_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.rfc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "#df['gbt_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.gbt.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "# df.head(20)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model mlp\n",
    "def call_model_with_mlp(sdf):\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    # create statistics dataframe\n",
    "    statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType()),\n",
    "        StructField(\"n_per_k\", ArrayType(IntegerType())),\n",
    "        StructField(\"mlp\", ArrayType(FloatType()))])\n",
    "\n",
    "    statistics = spark.createDataFrame([], statistics_schema)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_kmeans = prepare_for_kmeans(sdf_by_id)\n",
    "\n",
    "        # sdf_kmeans=pca_for_kmeans(sdf_kmeans)\n",
    "\n",
    "        #train_data,test_data = sdf_kmeans.randomSplit([0.7,0.3])\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans)\n",
    "        print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "        print(\"best k= \" + str(best_k))\n",
    "\n",
    "        transformed = kmeans_model.transform(\n",
    "            sdf_kmeans).select('uid', 'prediction', 'features')\n",
    "        # transformed.show()\n",
    "        sdf_join = transformed.join(sdf_by_id, on=['uid'], how='inner')\n",
    "        # sdf_join.show()\n",
    "\n",
    "        # define statistics variables\n",
    "        n_per_k = []\n",
    "        mlp_acc = []\n",
    "\n",
    "        for k in range(0, best_k):\n",
    "            temp_sdf = sdf_join.filter(sdf_join.prediction == k)\n",
    "            temp_sdf_malicious = create_malicious_df(temp_sdf)\n",
    "            temp_sdf_mixed = temp_sdf.union(temp_sdf_malicious)\n",
    "\n",
    "            sdf_mlp = prepare_for_mlp(temp_sdf_mixed)\n",
    "            # sdf_mlp.show()\n",
    "            train_data, test_data = sdf_mlp.randomSplit([0.7, 0.3])\n",
    "\n",
    "            acc = mlp(train_data, test_data)\n",
    "            mlp_acc.append(float(acc))\n",
    "\n",
    "            print('A MLP had an accuracy of: {0:2.2f}%'.format(acc*100))\n",
    "\n",
    "            n_per_k.append(int(temp_sdf.count()))\n",
    "\n",
    "        # update statistics\n",
    "        newRow_for_statistics = spark.createDataFrame(\n",
    "            [(str(i), int(best_k), float(silhouette), n_per_k, mlp_acc)])\n",
    "        statistics = statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return statistics\n",
    "\n",
    "# statistics=call_model_with_mlp(sdf)\n",
    "# statistics.show()\n",
    "\n",
    "# save\n",
    "#result_pdf = statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'statistics_tree_pca.pkl'))\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "# df.head()\n",
    "\n",
    "# output of model\n",
    "#df = statistics.select(\"*\").toPandas()\n",
    "#df['mlp_acc'] = [np.dot(df.n_per_k.to_numpy()[i],df.mlp.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_trees_k1 = pd.read_pickle(\n",
    "    os.path.join(BASE_PATH, 'all_statistics_trees_k1.pkl'))\n",
    "statistics_trees_k1.head()\n",
    "# statistics_trees_k1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_trees_k2 = pd.read_pickle(os.path.join(\n",
    "    BASE_PATH, 'all_statistics_trees_k2to5.pkl'))\n",
    "statistics_trees_k2['dtc_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['dtc_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['dtc_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "\n",
    "# statistics_trees_k2['rfc_acc_agg'].isnull().sum()\n",
    "\n",
    "# post processing\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_acc_agg']] = 0.8568376068376068\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_acc_agg']] = 0.8910256410256411\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_acc_agg']] = 0.9220183486238532\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_auroc_agg']] = 0.735998794040322\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_auroc_agg']] = 0.9531877968578725\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_auroc_agg']] = 0.9220183486238532\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_auprc_agg']] = 0.735998794040322\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_auprc_agg']] = 0.9531877968578725\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_auprc_agg']] = 0.9688447899695461\n",
    "\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['dtc_acc']] = 0.8008849557522124\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['rfc_acc']] = 0.834070796460177\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['gbt_acc']] = 0.8561946902654868\n",
    "\n",
    "statistics_trees_k2.head(5)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing\n",
    "best_result = statistics_trees_k1.join(\n",
    "    statistics_trees_k2, lsuffix='_1', rsuffix='_2')\n",
    "\n",
    "best_result[\"max_dtc_acc\"] = best_result[[\n",
    "    \"dtc_acc_1\", \"dtc_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_acc\"] = best_result[[\n",
    "    \"rfc_acc_1\", \"rfc_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_acc\"] = best_result[[\n",
    "    \"gbt_acc_1\", \"gbt_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_dtc_auprc\"] = best_result[[\n",
    "    \"dtc_auprc_1\", \"dtc_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_auprc\"] = best_result[[\n",
    "    \"rfc_auprc_1\", \"rfc_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_auprc\"] = best_result[[\n",
    "    \"gbt_auprc_1\", \"gbt_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_dtc_auroc\"] = best_result[[\n",
    "    \"dtc_auroc_1\", \"dtc_auroc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_auroc\"] = best_result[[\n",
    "    \"rfc_auroc_1\", \"rfc_auroc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_auroc\"] = best_result[[\n",
    "    \"gbt_auroc_1\", \"gbt_auroc_agg\"]].max(axis=1)\n",
    "\n",
    "\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['dtc_acc']] = 0.8325688073394495\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['rfc_acc']] = 0.8738532110091743\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['gbt_acc']] = 0.9220183486238532\n",
    "\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['dtc_acc']] = 0.8008849557522124\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['rfc_acc']] = 0.834070796460177\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['gbt_acc']] = 0.8561946902654868\n",
    "\n",
    "# best_result.describe()\n",
    "pd.set_option('display.max_columns', None)\n",
    "# best_result.head(10)\n",
    "# best_result['max_dtc_acc'].isnull().values.any()\n",
    "# best_result['rfc_acc'].isnull().sum()\n",
    "#index = best_result['rfc_acc'].index[best_result['rfc_acc'].apply(np.isnan)]\n",
    "FINAL = best_result[[\"max_dtc_acc\", \"max_dtc_auroc\", \"max_dtc_auprc\",\n",
    "                     \"max_rfc_acc\", \"max_rfc_auroc\", \"max_rfc_auprc\",\n",
    "                     \"max_gbt_acc\", \"max_gbt_auroc\", \"max_gbt_auprc\"]]\n",
    "FINAL.head()\n",
    "FINAL.describe()\n",
    "# print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector24 = pd.read_pickle(os.path.join(\n",
    "    BASE_PATH, 'all_statistics_trees_k1_24vevtor.pkl'))\n",
    "vector24.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "## **Useful Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe().show()\n",
    "# .printSchema()\n",
    "# .collect()\n",
    "# .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "## **Old**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "# #create json data from stored dataframe\n",
    "\n",
    "# def to_json(final):\n",
    "#     PERIOD=60\n",
    "\n",
    "#     data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "#     def date_to_str(o):\n",
    "#         if isinstance(o, datetime.datetime):\n",
    "#             return o.__str__()\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "#     L  = []\n",
    "\n",
    "#     import json\n",
    "#     import datetime\n",
    "#     import time\n",
    "\n",
    "#     r, c = data_for_json.shape\n",
    "#     for i in range(0, r):\n",
    "#         for j in range(0, c):\n",
    "#             data = {}\n",
    "#             data['id'] = data_for_json.columns.values[j]\n",
    "#             data['power'] = data_for_json.iloc[i][j]\n",
    "#             data['date']=data_for_json.index.tolist()[i]\n",
    "#             json_data = json.dumps(data,default=date_to_str)\n",
    "#             L.append(json_data)\n",
    "#             #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "#     return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load by schema\n",
    "\n",
    "# schema = StructType([\n",
    "#   StructField(\"num\", IntegerType()),\n",
    "#     StructField(\"date\", TimestampType()),\n",
    "#     StructField(\"id\", StringType()),\n",
    "#   StructField(\"power\", ArrayType(\n",
    "#       StructType([\n",
    "#           StructField(\"H0\", FloatType(), True),\n",
    "#           StructField(\"H1\", FloatType(), True),\n",
    "#           StructField(\"H2\", FloatType(), True),\n",
    "#           StructField(\"H3\", FloatType(), True),\n",
    "#           StructField(\"H4\", FloatType(), True),\n",
    "#           StructField(\"H5\", FloatType(), True),\n",
    "#           StructField(\"H6\", FloatType(), True),\n",
    "#           StructField(\"H7\", FloatType(), True),\n",
    "#           StructField(\"H8\", FloatType(), True),\n",
    "#           StructField(\"H9\", FloatType(), True),\n",
    "#           StructField(\"H10\", FloatType(), True),\n",
    "#           StructField(\"H11\", FloatType(), True),\n",
    "#           StructField(\"H12\", FloatType(), True),\n",
    "#           StructField(\"H13\", FloatType(), True),\n",
    "#           StructField(\"H14\", FloatType(), True),\n",
    "#           StructField(\"H15\", FloatType(), True),\n",
    "#           StructField(\"H16\", FloatType(), True),\n",
    "#           StructField(\"H17\", FloatType(), True),\n",
    "#           StructField(\"H18\", FloatType(), True),\n",
    "#           StructField(\"H19\", FloatType(), True),\n",
    "#           StructField(\"H20\", FloatType(), True),\n",
    "#           StructField(\"H21\", FloatType(), True),\n",
    "#           StructField(\"H22\", FloatType(), True),\n",
    "#           StructField(\"H23\", FloatType(), True)\n",
    "#       ])\n",
    "#    )\n",
    "#              )])\n",
    "\n",
    "# a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "# a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf\n",
    "\n",
    "# my_schema = StructType([\n",
    "#     StructField(\"id\", IntegerType()),\n",
    "#     StructField(\"age\", IntegerType())])\n",
    "# df=spark.read.csv(\"test.csv\", header=True,schema=my_schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# def plus_one(a):\n",
    "#     return a+1\n",
    "\n",
    "# plus_one_udf = pandas_udf(plus_one, returnType=IntegerType())\n",
    "\n",
    "# df.select(plus_one_udf(col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf (for array input)\n",
    "\n",
    "# df = spark.createDataFrame([([1,2,3,4,5,6],'val1'),([4,5,6,7,8,9],'val2')],['col1','col2'])\n",
    "# df.show()\n",
    "\n",
    "# @pandas_udf(ArrayType(LongType()))\n",
    "# def func(v):\n",
    "#     res=[]\n",
    "#     for row in v:\n",
    "#         temp=[]\n",
    "#         for k in range(len(row)):\n",
    "#             if (k<=2) or (k>4):\n",
    "#                 temp.append(row[k])\n",
    "#             else:\n",
    "#                 temp.append(row[k]*0)\n",
    "#         res.append(temp)\n",
    "#     return pd.Series(res)\n",
    "\n",
    "# df.withColumn('col3',func(df.col1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "377px",
    "left": "23px",
    "top": "138px",
    "width": "239.35px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
