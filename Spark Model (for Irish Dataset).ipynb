{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Imports and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=python3.6\n",
      "env: PYSPARK_PYTHON=python3.6\n",
      "env: ARROW_PRE_0_15_IPC_FORMAT=1\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, size, isnan, array_contains, when, count, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import pathlib\n",
    "import platform\n",
    "\n",
    "# env variables\n",
    "if platform.system() == 'Windows':\n",
    "    %env PYSPARK_DRIVER_PYTHON = python\n",
    "    %env PYSPARK_PYTHON = python\n",
    "elif platform.system() == 'Linux':\n",
    "    %env PYSPARK_DRIVER_PYTHON = python\n",
    "    %env PYSPARK_PYTHON = python3\n",
    "else:\n",
    "    %env PYSPARK_DRIVER_PYTHON = python3.6\n",
    "    %env PYSPARK_PYTHON = python3.6\n",
    "\n",
    "# incompatibility with Pyarrow\n",
    "# need to install Pyarrow 0.14.1 or lower or Set the environment variable ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "%env ARROW_PRE_0_15_IPC_FORMAT = 1\n",
    "\n",
    "# used versions:\n",
    "# spark='2.4.3' python='3.6' pyarrow='0.14.1'\n",
    "\n",
    "# for new system:\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# %pip install numpy\n",
    "# %pip install -U matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install Pyarrow==0.14.0\n",
    "# %env PYSPARK_DRIVER_PYTHON=python\n",
    "# %env PYSPARK_PYTHON=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Soroush/Desktop/Thesis/Code/Datasets/Irish/CER Electricity Revised March 2012\n",
      "/Users/Soroush/Desktop/Thesis/Code/Datasets/Irish/CER Electricity Revised March 2012/kmeans models\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "\n",
    "BASE_PATH = pathlib.Path().absolute()\n",
    "\n",
    "KMEANS_REL_PATH = os.path.join(os.path.join(os.path.join(\n",
    "    \"Datasets\", \"Irish\"), \"CER Electricity Revised March 2012\"), \"kmeans models\")\n",
    "DATASET_REL_PATH = os.path.join(\n",
    "    os.path.join(\"Datasets\", \"Irish\"), \"CER Electricity Revised March 2012\")\n",
    "\n",
    "\n",
    "DATASET_PATH = os.path.join(BASE_PATH, DATASET_REL_PATH)\n",
    "KMEANS_PATH = os.path.join(BASE_PATH, KMEANS_REL_PATH)\n",
    "\n",
    "print(DATASET_PATH)\n",
    "print(KMEANS_PATH)\n",
    "\n",
    "#from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "# load and save .read_pickle() and .to_pickle()\n",
    "\n",
    "# save\n",
    "# dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "# aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "# json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "# dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "# aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "# json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "# aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "# load\n",
    "#dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'dataset.pkl'))\n",
    "#aggregated_dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'aggregated_dataset.pkl'))\n",
    "\n",
    "#.pkl to .csv\n",
    "#dataset = pd.read_pickle(os.path.join(DATASET_PATH, 'irish_aggregated_dataset.pkl'))\n",
    "#dataset.to_csv(os.path.join(DATASET_PATH, 'irish_aggregated_dataset.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **Malicious Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate malicious samples\n",
    "def h1(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    alpha = random.uniform(MIN, MAX)\n",
    "    temp = np.array(x)\n",
    "    return (temp*alpha).tolist()\n",
    "\n",
    "\n",
    "def h2(x):\n",
    "    MIN_OFF = 4  # hour\n",
    "    DURATION = random.randint(MIN_OFF, 23)\n",
    "    START = random.randint(0, 23-DURATION) if DURATION != 23 else 0\n",
    "    END = START+DURATION\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        if i < START or i >= END:\n",
    "            temp.append(x[i])\n",
    "        else:\n",
    "            temp.append(0.0)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h3(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(x[i]*random.uniform(MIN, MAX))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h4(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    mean = np.mean(x)\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean*random.uniform(MIN, MAX))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h5(x):\n",
    "    MAX = 0.8\n",
    "    MIN = 0.1\n",
    "    mean = np.mean(x)\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(mean)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def h6(x):\n",
    "    temp = np.array(x)\n",
    "    # temp=temp[::-1]\n",
    "    temp = np.flipud(temp)\n",
    "    return temp.tolist()\n",
    "\n",
    "\n",
    "# add malicious samples\n",
    "def create_malicious_df(sdf):\n",
    "    def random_attack_assigner(x):\n",
    "        NUMBER_OF_MALICIOUS_GENERATOR = 6\n",
    "        res = []\n",
    "        for row in x:\n",
    "            rand = random.randint(1, NUMBER_OF_MALICIOUS_GENERATOR)\n",
    "            if rand == 1:\n",
    "                temp = (h1(row))\n",
    "            elif rand == 2:\n",
    "                temp = (h2(row))\n",
    "            elif rand == 3:\n",
    "                temp = (h3(row))\n",
    "            elif rand == 4:\n",
    "                temp = (h4(row))\n",
    "            elif rand == 5:\n",
    "                temp = (h5(row))\n",
    "            elif rand == 6:\n",
    "                temp = (h6(row))\n",
    "            res.append(temp)\n",
    "        return pd.Series(res)\n",
    "    random_attack_assigner_UDF = pandas_udf(\n",
    "        random_attack_assigner, returnType=ArrayType(FloatType()))\n",
    "    # sdf_malicious=sdf\n",
    "    N = False\n",
    "    sdf = sdf.withColumn(\"N\", f.lit(N))  # malicious sample\n",
    "    # change '#' column number to negative\n",
    "    sdf = sdf.withColumn(\"#\", col(\"#\")*-1)\n",
    "    sdf = sdf.withColumn(\"power\", random_attack_assigner_UDF(col(\"power\")))\n",
    "    # sdf=sdf.drop('statistics')\n",
    "    sdf = sdf.withColumn(\"statistics\", generate_feature_UDF(col(\"power\")))\n",
    "    sdf = add_statistics_column(sdf)  # for update statistics\n",
    "    return sdf.select(sdf.columns)  # to reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "def plot_malicious_samples():\n",
    "\n",
    "    read_value = [3.4803431034088135, 2.529871702194214, 2.2175486087799072, 2.629481077194214, 2.9629790782928467, 2.0697860717773438, 2.900712251663208, 2.926414966583252, 4.8191237449646, 4.156486988067627, 2.6474769115448, 2.1933677196502686,\n",
    "                  2.261159658432007, 2.340345621109009, 2.7386586666107178, 3.2414891719818115, 1.8946533203125, 3.1397650241851807, 2.8951449394226074, 3.4589333534240723, 2.726524829864502, 6.511429309844971, 3.4918391704559326, 3.787257432937622]\n",
    "    lists = []\n",
    "    colors = ['b', 'r-', 'g--', 'c:', 'm-.', 'y-', 'k--']\n",
    "    lists.append(read_value)\n",
    "    lists.append(h1(read_value))\n",
    "    lists.append(h2(read_value))\n",
    "    lists.append(h3(read_value))\n",
    "    lists.append(h4(read_value))\n",
    "    lists.append(h5(read_value))\n",
    "    lists.append(h6(read_value))\n",
    "    #font = {'size': 12}\n",
    "    #plt.rc('font', **font)\n",
    "    plt.figure(num=None, figsize=(14, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.xlabel(\"time (hour)\", fontsize=18)\n",
    "    plt.ylabel(\"usage (kw)\", fontsize=18)\n",
    "    #plt.title(\"malicious samples\")\n",
    "    plt.xticks(np.arange(0, 24, step=1))\n",
    "    plt.plot(read_value)\n",
    "    for i in range(len(lists)):\n",
    "        if i == 0:\n",
    "            plt.plot(lists[i], colors[i], label='normal usage')\n",
    "        else:\n",
    "            plt.plot(lists[i], colors[i], label='attack %s' % i)\n",
    "    # plt.legend()\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.savefig('attack.pdf', bbox_inches='tight')\n",
    "    # plt.savefig('attack.eps', format='eps')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# plot_malicious_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "## **Prepare Spark Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "def rename_dataframe(sdf):\n",
    "    names = ['#', 'id', 'date', 'power']\n",
    "    for c, n in zip(sdf.columns, names):\n",
    "        sdf = sdf.withColumnRenamed(c, n)\n",
    "    return sdf\n",
    "\n",
    "# sdf=rename_dataframe(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# convert power to array\n",
    "\n",
    "\n",
    "def string_power_to_array(sdf):\n",
    "    temp = sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"), \"\\\\]\", \"\")\n",
    "                          .alias(\"power\"))\n",
    "    temp = temp.withColumn(\"power\", split(col(\"power\"), \",\\s*\")\n",
    "                           .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "# sdf=string_power_to_array(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "def add_validation_column(sdf):\n",
    "    def validation(x):\n",
    "        res = []\n",
    "        for row in x:\n",
    "            v = True\n",
    "            if (len(row) != 24 or  # unusual size\n",
    "                (row >= 0).sum() != 24 or  # number of valid elements = 24\n",
    "                # sum(n >= 0 for n in row) != 24 or\n",
    "                # equal or more than 3 zero elements\n",
    "                np.count_nonzero(row == 0) >= 3 or\n",
    "                    sum(n < 0 for n in row) > 0):  # not have negative element\n",
    "                v = False\n",
    "            res.append(v)\n",
    "        return pd.Series(res)\n",
    "    validation_UDF = pandas_udf(validation, returnType=BooleanType())\n",
    "    temp = sdf.withColumn(\"V\", validation_UDF(col(\"power\")))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_validation_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "# add \"N\"ormal consumption (\"N\"onmalicious) column\n",
    "def add_Normal_column(sdf):\n",
    "    N = True\n",
    "    temp = sdf.withColumn(\"N\", f.lit(N))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_Normal_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# filter data\n",
    "def filter_dataset(sdf, from_date=\"BEGIN\", to_date=\"END\", ID=\"*\", V=\"*\"):\n",
    "    temp = sdf\n",
    "    if (from_date != \"BEGIN\"):\n",
    "        temp = temp.filter(sdf.date > from_date)  # filter date (from X)\n",
    "    if (to_date != \"END\"):\n",
    "        temp = temp.filter(sdf.date < to_date)  # filter date (to Y)\n",
    "    if (ID != \"*\"):\n",
    "        temp = temp.filter(sdf.id == ID)  # filter IDs\n",
    "    if (V != \"*\"):\n",
    "        temp = temp.filter(sdf.V == V)  # filter validation\n",
    "    return temp\n",
    "\n",
    "# sdf=filter_dataset(sdf,from_date=\"BEGIN\",to_date=\"END\",ID=\"Apt36\",V=\"True\")\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "def split_power(sdf):\n",
    "    temp = sdf.select(\"#\", \"date\", \"id\",\n",
    "                      sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                          \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                      sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                          \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                      sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                          \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                      sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                          \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                      sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                          \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                      sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"))\n",
    "    return temp\n",
    "\n",
    "# split_sdf=split_power(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "# make id list\n",
    "def get_ids(sdf):\n",
    "    IDs = np.array(sdf.select(\"id\").distinct().collect())\n",
    "    #IDs = IDs.reshape(1, len(IDs))\n",
    "    IDs = IDs.flatten() \n",
    "    return IDs\n",
    "\n",
    "# number of instances per id\n",
    "def get_instance_number_per_id(sdf):\n",
    "    IDs = np.array(sdf.select(\"id\").distinct().collect())\n",
    "    IDs = IDs.flatten() \n",
    "    id_instance_dict =\t{}\n",
    "    \n",
    "    for i in IDs:\n",
    "        temp = sdf.filter(sdf.id == int(i))\n",
    "        row_number = temp.count()\n",
    "        id_instance_dict[int(i)] = row_number\n",
    "    \n",
    "    return id_instance_dict\n",
    "\n",
    "\n",
    "\n",
    "# generate uniqe id\n",
    "def generate_uniqe_id(sdf):\n",
    "    temp = sdf\n",
    "    temp = temp.withColumn(\"uid\", f.concat(\n",
    "        col(\"id\"), f.lit(\"-\"), col(\"#\")).alias(\"uid\"))\n",
    "    return temp\n",
    "\n",
    "# sdf=generate_uniqe_id(sdf)\n",
    "\n",
    "\n",
    "# generate feature\n",
    "def generate_feature(x):\n",
    "    res = []\n",
    "    for row in x:\n",
    "        row = np.array(row)  # to numpy\n",
    "        statistics = []\n",
    "        min_val = np.nanmin(row)\n",
    "        max_val = np.nanmax(row)\n",
    "        mean_val = np.nanmean(row)\n",
    "        std_val = np.nanstd(row)\n",
    "        statistics.append(mean_val)\n",
    "        statistics.append(std_val)\n",
    "        statistics.append(min_val)\n",
    "        statistics.append(max_val)\n",
    "        res.append(statistics)\n",
    "    return pd.Series(res)\n",
    "\n",
    "\n",
    "generate_feature_UDF = pandas_udf(\n",
    "    generate_feature, returnType=ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "def add_statistics_column(sdf):\n",
    "    temp = sdf.withColumn(\"statistics\", generate_feature_UDF(col(\"power\")))\n",
    "    return temp\n",
    "\n",
    "# sdf=add_statistics_column(sdf)\n",
    "# sdf.show()\n",
    "\n",
    "\n",
    "# sdf.show()\n",
    "# print(\"number of rows: \" + str(sdf.count()))\n",
    "# sdf.collect()\n",
    "# sdf.printSchema()\n",
    "# split_sdf=add_validation_column(split_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "## **K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_kmeans(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    temp = generate_uniqe_id(temp)\n",
    "\n",
    "    # make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_kmeans = vecAssembler.transform(\n",
    "        temp).select(col(\"uid\"), col(\"features\"))\n",
    "    return df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means\n",
    "\n",
    "\n",
    "def kmeans(sdf_kmeans):\n",
    "    # find best k\n",
    "    MAX_k = 3\n",
    "    costs = np.zeros(MAX_k)\n",
    "    silhouettes = np.zeros(MAX_k)\n",
    "    silhouettes[1] = 0  # set value for k=1\n",
    "    for k in range(2, MAX_k):\n",
    "        kmeans = KMeans().setK(k).setSeed(1)\n",
    "        model = kmeans.fit(sdf_kmeans)\n",
    "        costs[k] = model.computeCost(sdf_kmeans)  # requires Spark 2.0 or later\n",
    "        predictions = model.transform(sdf_kmeans)\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        silhouettes[k] = evaluator.evaluate(predictions)\n",
    "\n",
    "    # show silhouette\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(2, MAX_k), silhouettes[2:MAX_k])\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('silhouette')\n",
    "\n",
    "    # show cost\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(range(2, MAX_k), costs[2:MAX_k])\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('cost')\n",
    "\n",
    "    # find best k\n",
    "    best_k = np.argmax(silhouettes)\n",
    "    print(\"maximum value of silhouette is: \" +\n",
    "          str(silhouettes[best_k]) + \" in index: \" + str(best_k))\n",
    "\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(best_k).setSeed(1)\n",
    "    model = kmeans.fit(sdf_kmeans)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(sdf_kmeans)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "#     print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "#     # Shows the result.\n",
    "#     centers = model.clusterCenters()\n",
    "#     print(\"Cluster Centers: \")\n",
    "#     for center in centers:\n",
    "#         print(center)\n",
    "\n",
    "#     transformed = model.transform(sdf_kmeans).select('id', 'prediction')\n",
    "#     transformed.show()\n",
    "#     transformed.groupby('prediction').count().show()\n",
    "#     rows = transformed.collect()\n",
    "#     prediction = spark.createDataFrame(rows)\n",
    "#     prediction.show()\n",
    "    return model, best_k, silhouette  # silhouettes: new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **Decision Tree Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_decision_tree_methods(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # boolean to string (for \"N\" column)\n",
    "    temp = temp.withColumn(\"N\", f.col(\"N\").cast('string'))\n",
    "    # temp.printSchema()\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    # temp=generate_uniqe_id(temp)\n",
    "\n",
    "    # make features ready\n",
    "    assembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    output = assembler.transform(temp)\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "\n",
    "    # make label ready\n",
    "    indexer = StringIndexer(inputCol=\"N\", outputCol=\"NIndex\")\n",
    "    output_fixed = indexer.fit(output).transform(output)\n",
    "\n",
    "    final_data = output_fixed.select(\"features\", 'NIndex')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run decision tree methods\n",
    "def decision_tree(train_data, test_data):\n",
    "    dtc = DecisionTreeClassifier(labelCol='NIndex', featuresCol='features')\n",
    "    rfc = RandomForestClassifier(\n",
    "        labelCol='NIndex', featuresCol='features')  # ,numTrees=100\n",
    "    gbt = GBTClassifier(labelCol='NIndex', featuresCol='features')\n",
    "\n",
    "    dtc_model = dtc.fit(train_data)\n",
    "    rfc_model = rfc.fit(train_data)\n",
    "    gbt_model = gbt.fit(train_data)\n",
    "\n",
    "    dtc_predictions = dtc_model.transform(test_data)\n",
    "    rfc_predictions = rfc_model.transform(test_data)\n",
    "    gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "    # evaluation\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"NIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "    rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "    gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "    # new:\n",
    "    # Let's use the run-of-the-mill evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='NIndex')\n",
    "    # We have only two choices: area under ROC and PR curves :-(\n",
    "    dtc_auroc = evaluator.evaluate(\n",
    "        dtc_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    dtc_auprc = evaluator.evaluate(\n",
    "        dtc_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    rfc_auroc = evaluator.evaluate(\n",
    "        rfc_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    rfc_auprc = evaluator.evaluate(\n",
    "        rfc_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    gbt_auroc = evaluator.evaluate(\n",
    "        gbt_predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    gbt_auprc = evaluator.evaluate(\n",
    "        gbt_predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    #print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "    #print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "    #print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "    #print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "    #print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "    #print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "    #print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "    #print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "    #print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n",
    "\n",
    "    return dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pca(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split_power(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split_power(temp)\n",
    "\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    temp = generate_uniqe_id(temp)\n",
    "\n",
    "    # make ready\n",
    "    vecAssembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "    df_pca = vecAssembler.transform(temp).select(\n",
    "        \"#\", \"V\", \"N\", \"date\", \"id\", \"uid\", col(\"features\"))\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "def pca_for_tree(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\n",
    "        \"NIndex\", col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "def pca_for_kmeans(sdf):\n",
    "    #sdf = prepare_for_pca(sdf)\n",
    "    pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(sdf)\n",
    "    result = model.transform(sdf).select(\n",
    "        \"uid\", col(\"pcaFeatures\").alias(\"features\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "# sdf_pca=pca(sdf)\n",
    "# sdf_pca.show() #truncate=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "## **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mlp(sdf):\n",
    "\n",
    "    temp = sdf\n",
    "\n",
    "    # define function for split power column\n",
    "    def split(sdf):\n",
    "        temp = sdf.select(\"#\", \"V\", \"N\", \"date\", \"id\", \"uid\",\n",
    "                          sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\n",
    "                              \"H1\"), sdf.power[2].alias(\"H2\"), sdf.power[3].alias(\"H3\"),\n",
    "                          sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\n",
    "                              \"H5\"), sdf.power[6].alias(\"H6\"), sdf.power[7].alias(\"H7\"),\n",
    "                          sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\n",
    "                              \"H9\"), sdf.power[10].alias(\"H10\"), sdf.power[11].alias(\"H11\"),\n",
    "                          sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\n",
    "                              \"H13\"), sdf.power[14].alias(\"H14\"), sdf.power[15].alias(\"H15\"),\n",
    "                          sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\n",
    "                              \"H17\"), sdf.power[18].alias(\"H18\"), sdf.power[19].alias(\"H19\"),\n",
    "                          sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\n",
    "                              \"H21\"), sdf.power[22].alias(\"H22\"), sdf.power[23].alias(\"H23\"),\n",
    "                          sdf.statistics[0].alias(\"S0\"), sdf.statistics[1].alias(\"S1\"), sdf.statistics[2].alias(\"S2\"), sdf.statistics[3].alias(\"S3\"))\n",
    "        return temp\n",
    "\n",
    "    # call the split_power function\n",
    "    temp = split(temp)\n",
    "\n",
    "    # boolean to string (for \"N\" column)\n",
    "    temp = temp.withColumn(\"N\", f.col(\"N\").cast('string'))\n",
    "    # temp.printSchema()\n",
    "\n",
    "    # filter date\n",
    "    # temp=temp.filter(temp.date > \"2014-08-15\").filter(temp.date < \"2014-08-19\") #filter dates\n",
    "    # temp=temp.filter(temp.id == \"Apt40\") #filter IDs\n",
    "    temp = temp.filter(temp.V == True)  # filter valid rows\n",
    "\n",
    "    FEATURES = ['H0', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11',\n",
    "                'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'S0', 'S1', 'S2', 'S3']\n",
    "\n",
    "    # call the generate_uniqe_id function\n",
    "    # temp=generate_uniqe_id(temp)\n",
    "\n",
    "    # make features ready\n",
    "    assembler = VectorAssembler(inputCols=FEATURES, outputCol=\"features\")\n",
    "    output = assembler.transform(temp)\n",
    "    #df_kmeans = vecAssembler.transform(temp).select(col(\"uid\").alias(\"id\"), col(\"features\"))\n",
    "\n",
    "    # make label ready\n",
    "    indexer = StringIndexer(inputCol=\"N\", outputCol=\"label\")\n",
    "    output_fixed = indexer.fit(output).transform(output)\n",
    "\n",
    "    final_data = output_fixed.select(\"features\", 'label')\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run mlp method\n",
    "def mlp(train_data, test_data, layers=[28, 50, 10, 2]):\n",
    "    # specify layers for the neural network:\n",
    "    # input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "    # and output of size 3 (classes)\n",
    "\n",
    "    # create the trainer and set its parameters\n",
    "    trainer = MultilayerPerceptronClassifier(\n",
    "        maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "    # train_data.show()\n",
    "    # train the model\n",
    "    model = trainer.fit(train_data)\n",
    "\n",
    "    # compute accuracy on the test set\n",
    "    result = model.transform(test_data)\n",
    "    predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        metricName=\"accuracy\")  # BinaryClassificationEvaluator\n",
    "    acc = evaluator.evaluate(predictionAndLabels)\n",
    "    print(\"Test set accuracy = \" + str(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1088 1238 1342 ... 6721 6819 7166]\n"
     ]
    }
   ],
   "source": [
    "# create SparkSession\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"anomaly_detection\").master(\"local[20]\").getOrCreate()\n",
    "# network problem? type it in commandline: sudo hostname -s 127.0.0.1\n",
    "\n",
    "# define schema\n",
    "schema = StructType([\n",
    "    StructField(\"#\", IntegerType()),\n",
    "    StructField(\"id\", TimestampType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"power\", StringType())])\n",
    "\n",
    "# read data\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema=True, schema=schema).load(\n",
    "    os.path.join(DATASET_PATH, 'irish_aggregated_dataset.csv'))\n",
    "\n",
    "\n",
    "sdf = rename_dataframe(sdf)\n",
    "sdf = string_power_to_array(sdf)\n",
    "sdf = add_statistics_column(sdf)\n",
    "sdf = add_validation_column(sdf)\n",
    "sdf = add_Normal_column(sdf)\n",
    "#sdf = filter_dataset(sdf, from_date=\"BEGIN\",to_date=\"END\", ID=\"1000\", V=\"True\")\n",
    "sdf = filter_dataset(sdf, from_date=\"2010-07-15\",to_date=\"2010-07-21\", ID=\"*\", V=\"True\")\n",
    "sdf = generate_uniqe_id(sdf)\n",
    "\n",
    "# ids\n",
    "id_list = get_ids(sdf)\n",
    "\n",
    "# number of instances per id\n",
    "dic_instance_number_per_id = get_instance_number_per_id(sdf)\n",
    "\n",
    "\n",
    "# #dataset\n",
    "# print(\"dataframe schema:\")\n",
    "# print(\"number of rows: \" + str(sdf.count()))\n",
    "# sdf.printSchema()\n",
    "#print(\"benign dataframe (sdf):\")\n",
    "#sdf.show()\n",
    "\n",
    "# #generate malicious data\n",
    "#sdf_malicious = create_malicious_df(sdf)\n",
    "#print(\"malicious dataframe (sdf_malicious):\")\n",
    "#sdf_malicious.show()\n",
    "\n",
    "#sdf_mix = sdf.union(sdf_malicious)\n",
    "#print(\"final dataframe (sdf_mix):\")\n",
    "#sdf_mix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a row\n",
    "# sdf.collect()[1][3] #[8027][3] is used for figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- k-means started!\n",
      "customer 1: 1000\n",
      "maximum value of silhouette is: 0.3108565158404826 in index: 2\n",
      "AAAAAAAAAA\n",
      "+----+---+------------------+\n",
      "|  id|  k|        Silhouette|\n",
      "+----+---+------------------+\n",
      "|1000|  2|0.3108565158404826|\n",
      "+----+---+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kmeans separately\n",
    "\n",
    "\n",
    "def call_kmeans(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    kmeans_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType())])\n",
    "\n",
    "    kmeans_statistics = spark.createDataFrame([], kmeans_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "    # replace sdf with final_sdf for clustring benign and malicious data\n",
    "    sdf_kmeans = prepare_for_kmeans(sdf)\n",
    "    # sdf_kmeans=pca_for_kmeans(sdf_kmeans) #0.8725788926917551 to 0.9101118371931005\n",
    "    # sdf_kmeans.show()\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_kmeans_by_id = sdf_kmeans.filter(\n",
    "            sdf_kmeans.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "        # sdf_kmeans_by_id.show()\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans_by_id)\n",
    "        # kmeans_model.save(os.path.join(KMEANS_PATH,str(i)))\n",
    "        summary = kmeans_model.summary\n",
    "        if summary.clusterSizes[1] > 200:\n",
    "            print(\"AAAAAAAAAA\")\n",
    "        else:\n",
    "            print(\"BBBBBBBBBB\")\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame(\n",
    "            [(str(i), int(best_k), float(silhouette))])\n",
    "        kmeans_statistics = kmeans_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "        # model_name = KMeansModel.load(os.path.join(KMEANS_PATH,str(i)) #for load model\n",
    "    return kmeans_statistics\n",
    "\n",
    "\n",
    "print(\"-------------------- k-means started!\")\n",
    "kmeans_statistics = call_kmeans(sdf)\n",
    "kmeans_statistics.show()\n",
    "# save\n",
    "# result_pdf = kmeans_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'kmeans_statistics.pkl'))\n",
    "# df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree separately\n",
    "def call_trees(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    trees_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"dtc_acc\", FloatType()),\n",
    "        StructField(\"dtc_auroc\", FloatType()),\n",
    "        StructField(\"dtc_auprc\", FloatType()),\n",
    "        StructField(\"rfc_acc\", FloatType()),\n",
    "        StructField(\"rfc_auroc\", FloatType()),\n",
    "        StructField(\"rfc_auprc\", FloatType()),\n",
    "        StructField(\"gbt_acc\", FloatType()),\n",
    "        StructField(\"gbt_auroc\", FloatType()),\n",
    "        StructField(\"gbt_auprc\", FloatType())])\n",
    "\n",
    "    trees_statistics = spark.createDataFrame([], trees_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_trees_by_id = sdf.filter(\n",
    "            sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_trees_by_id_malicious = create_malicious_df(sdf_trees_by_id)\n",
    "        sdf_trees_by_id_mixed = sdf_trees_by_id.union(\n",
    "            sdf_trees_by_id_malicious)\n",
    "\n",
    "        # sdf_trees=prepare_for_decision_tree_methods(sdf)\n",
    "\n",
    "        sdf_trees = prepare_for_decision_tree_methods(sdf_trees_by_id_mixed)\n",
    "        train_data, test_data = sdf_trees.randomSplit([0.7, 0.3])\n",
    "\n",
    "        dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc = decision_tree(\n",
    "            train_data, test_data)\n",
    "\n",
    "        print('A single decision tree had an accuracy of: {0:2.2f}%'.format(\n",
    "            dtc_acc*100))\n",
    "        print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "        print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "        print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(\n",
    "            rfc_acc*100))\n",
    "        print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "        print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "        print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(\n",
    "            gbt_acc*100))\n",
    "        print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "        print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), float(dtc_acc), float(dtc_auroc), float(dtc_auprc),\n",
    "                                                        float(rfc_acc), float(\n",
    "                                                            rfc_auroc), float(rfc_auprc),\n",
    "                                                        float(gbt_acc), float(gbt_auroc), float(gbt_auprc))])\n",
    "        trees_statistics = trees_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "    return trees_statistics\n",
    "\n",
    "\n",
    "#print(\"-------------------- decision tree started!\")\n",
    "# trees_statistics=call_trees(sdf)\n",
    "# trees_statistics.show()\n",
    "# save\n",
    "# result_pdf = trees_statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'trees_statistics.pkl'))\n",
    "# load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'all_statistics_trees_k1.pkl'))\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp separetely\n",
    "def call_mlp(sdf):\n",
    "\n",
    "    # create statistics dataframe\n",
    "    mlp_statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"mlp\", FloatType())])\n",
    "\n",
    "    mlp_statistics = spark.createDataFrame([], mlp_statistics_schema)\n",
    "\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_trees_by_id = sdf.filter(\n",
    "            sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_mlp = prepare_for_mlp(sdf)\n",
    "        train_data, test_data = sdf_mlp.randomSplit([0.7, 0.3])\n",
    "\n",
    "        acc = mlp(train_data, test_data, [28, 60, 10, 2])\n",
    "\n",
    "        print('A MLP had an accuracy of: {0:2.2f}%'.format(acc*100))\n",
    "\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), float(acc))])\n",
    "        mlp_statistics = mlp_statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "    return mlp_statistics\n",
    "\n",
    "#print(\"-------------------- mlp started!\")\n",
    "# run\n",
    "# mlp_statistics=call_mlp(sdf_mix)\n",
    "# mlp_statistics.show()\n",
    "\n",
    "# save\n",
    "#df = statistics.select(\"*\").toPandas()\n",
    "#df.to_pickle(os.path.join(BASE_PATH, 'mlp_statistics.pkl'))\n",
    "\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'mlp_statistics.pkl'))\n",
    "# df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model tree\n",
    "def call_model_with_tree(sdf):\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    # create statistics dataframe\n",
    "    statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType()),\n",
    "        StructField(\"n_per_k\", ArrayType(IntegerType())),\n",
    "        StructField(\"dtc_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"dtc_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"dtc_auprc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"rfc_auprc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_acc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_auroc\", ArrayType(FloatType())),\n",
    "        StructField(\"gbt_auprc\", ArrayType(FloatType()))])\n",
    "\n",
    "    statistics = spark.createDataFrame([], statistics_schema)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_kmeans = prepare_for_kmeans(sdf_by_id)\n",
    "\n",
    "        # sdf_kmeans=pca_for_kmeans(sdf_kmeans)\n",
    "\n",
    "        #train_data,test_data = sdf_kmeans.randomSplit([0.7,0.3])\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans)\n",
    "        print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "        print(\"best k= \" + str(best_k))\n",
    "\n",
    "        transformed = kmeans_model.transform(\n",
    "            sdf_kmeans).select('uid', 'prediction', 'features')\n",
    "        # transformed.show()\n",
    "        sdf_join = transformed.join(sdf_by_id, on=['uid'], how='inner')\n",
    "        # sdf_join.show()\n",
    "\n",
    "        # define statistics variables\n",
    "        n_per_k = []\n",
    "        dtc_acc_list = []\n",
    "        dtc_auroc_list = []\n",
    "        dtc_auprc_list = []\n",
    "        rfc_acc_list = []\n",
    "        rfc_auroc_list = []\n",
    "        rfc_auprc_list = []\n",
    "        gbt_acc_list = []\n",
    "        gbt_auroc_list = []\n",
    "        gbt_auprc_list = []\n",
    "\n",
    "        for k in range(0, best_k):\n",
    "            temp_sdf = sdf_join.filter(sdf_join.prediction == k)\n",
    "            temp_sdf_malicious = create_malicious_df(temp_sdf)\n",
    "            temp_sdf_mixed = temp_sdf.union(temp_sdf_malicious)\n",
    "            tree_data = prepare_for_decision_tree_methods(temp_sdf_mixed)\n",
    "\n",
    "            # tree_data=pca_for_tree(tree_data)\n",
    "\n",
    "            train_data, test_data = tree_data.randomSplit([0.7, 0.3])\n",
    "            dtc_acc, dtc_auroc, dtc_auprc, rfc_acc, rfc_auroc, rfc_auprc, gbt_acc, gbt_auroc, gbt_auprc = decision_tree(\n",
    "                train_data, test_data)\n",
    "            print('A single decision tree had an accuracy of: {0:2.2f}%'.format(\n",
    "                dtc_acc*100))\n",
    "            print(\"DT Area under ROC Curve: {:.4f}\".format(dtc_auroc))\n",
    "            print(\"DT Area under PR Curve: {:.4f}\".format(dtc_auprc))\n",
    "            print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(\n",
    "                rfc_acc*100))\n",
    "            print(\"RF Area under ROC Curve: {:.4f}\".format(rfc_auroc))\n",
    "            print(\"RF Area under PR Curve: {:.4f}\".format(rfc_auprc))\n",
    "            print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(\n",
    "                gbt_acc*100))\n",
    "            print(\"GB Area under ROC Curve: {:.4f}\".format(gbt_auroc))\n",
    "            print(\"GB Area under PR Curve: {:.4f}\".format(gbt_auprc))\n",
    "\n",
    "            n_per_k.append(int(temp_sdf.count()))\n",
    "            dtc_acc_list.append(float(dtc_acc))\n",
    "            dtc_auroc_list.append(float(dtc_auroc))\n",
    "            dtc_auprc_list.append(float(dtc_auprc))\n",
    "            rfc_acc_list.append(float(rfc_acc))\n",
    "            rfc_auroc_list.append(float(rfc_auroc))\n",
    "            rfc_auprc_list.append(float(rfc_auprc))\n",
    "            gbt_acc_list.append(float(gbt_acc))\n",
    "            gbt_auroc_list.append(float(gbt_auroc))\n",
    "            gbt_auprc_list.append(float(gbt_auprc))\n",
    "\n",
    "        # update statistics\n",
    "        newRow_for_statistics = spark.createDataFrame([(str(i), int(best_k), float(silhouette), n_per_k,\n",
    "                                                        dtc_acc_list, dtc_auroc_list, dtc_auprc_list,\n",
    "                                                        rfc_acc_list, rfc_auroc_list, rfc_auprc_list,\n",
    "                                                        gbt_acc_list, gbt_auroc_list, gbt_auprc_list)])\n",
    "        statistics = statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return statistics\n",
    "\n",
    "# statistics=call_model_with_tree(sdf)\n",
    "# statistics.show()\n",
    "\n",
    "# save\n",
    "# result_pdf = statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "\n",
    "\n",
    "# load\n",
    "df = pd.read_pickle(os.path.join(BASE_PATH, 'all_statistics_trees_k2to5.pkl'))\n",
    "df.head()\n",
    "\n",
    "# output of model\n",
    "#df['dtc_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.dtc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "#df['rfc_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.rfc.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "#df['gbt_acc_agg'] = [np.dot(df.n_per_k.to_numpy()[i],df.gbt.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "# df.head(20)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model mlp\n",
    "def call_model_with_mlp(sdf):\n",
    "    id_list = get_ids(sdf)\n",
    "\n",
    "    # create statistics dataframe\n",
    "    statistics_schema = StructType([\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"k\", IntegerType()),\n",
    "        StructField(\"Silhouette\", FloatType()),\n",
    "        StructField(\"n_per_k\", ArrayType(IntegerType())),\n",
    "        StructField(\"mlp\", ArrayType(FloatType()))])\n",
    "\n",
    "    statistics = spark.createDataFrame([], statistics_schema)\n",
    "\n",
    "    iteration = 1\n",
    "    for i in np.nditer(id_list):\n",
    "        sdf_by_id = sdf.filter(sdf.uid.like(str(i)+\"-\"+\"%\"))  # filter IDs\n",
    "        print(\"customer \" + str(iteration)+\": \" + str(i))\n",
    "\n",
    "        sdf_kmeans = prepare_for_kmeans(sdf_by_id)\n",
    "\n",
    "        # sdf_kmeans=pca_for_kmeans(sdf_kmeans)\n",
    "\n",
    "        #train_data,test_data = sdf_kmeans.randomSplit([0.7,0.3])\n",
    "        kmeans_model, best_k, silhouette = kmeans(sdf_kmeans)\n",
    "        print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "        print(\"best k= \" + str(best_k))\n",
    "\n",
    "        transformed = kmeans_model.transform(\n",
    "            sdf_kmeans).select('uid', 'prediction', 'features')\n",
    "        # transformed.show()\n",
    "        sdf_join = transformed.join(sdf_by_id, on=['uid'], how='inner')\n",
    "        # sdf_join.show()\n",
    "\n",
    "        # define statistics variables\n",
    "        n_per_k = []\n",
    "        mlp_acc = []\n",
    "\n",
    "        for k in range(0, best_k):\n",
    "            temp_sdf = sdf_join.filter(sdf_join.prediction == k)\n",
    "            temp_sdf_malicious = create_malicious_df(temp_sdf)\n",
    "            temp_sdf_mixed = temp_sdf.union(temp_sdf_malicious)\n",
    "\n",
    "            sdf_mlp = prepare_for_mlp(temp_sdf_mixed)\n",
    "            # sdf_mlp.show()\n",
    "            train_data, test_data = sdf_mlp.randomSplit([0.7, 0.3])\n",
    "\n",
    "            acc = mlp(train_data, test_data)\n",
    "            mlp_acc.append(float(acc))\n",
    "\n",
    "            print('A MLP had an accuracy of: {0:2.2f}%'.format(acc*100))\n",
    "\n",
    "            n_per_k.append(int(temp_sdf.count()))\n",
    "\n",
    "        # update statistics\n",
    "        newRow_for_statistics = spark.createDataFrame(\n",
    "            [(str(i), int(best_k), float(silhouette), n_per_k, mlp_acc)])\n",
    "        statistics = statistics.union(newRow_for_statistics)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return statistics\n",
    "\n",
    "# statistics=call_model_with_mlp(sdf)\n",
    "# statistics.show()\n",
    "\n",
    "# save\n",
    "#result_pdf = statistics.select(\"*\").toPandas()\n",
    "# result_pdf.to_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "\n",
    "# load\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'statistics_tree_pca.pkl'))\n",
    "#df = pd.read_pickle(os.path.join(BASE_PATH, 'statistics.pkl'))\n",
    "# df.head()\n",
    "\n",
    "# output of model\n",
    "#df = statistics.select(\"*\").toPandas()\n",
    "#df['mlp_acc'] = [np.dot(df.n_per_k.to_numpy()[i],df.mlp.to_numpy()[i])/sum(df.n_per_k.to_numpy()[i]) for i in range(0,len(df.n_per_k.to_numpy()))]\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_trees_k1 = pd.read_pickle(\n",
    "    os.path.join(BASE_PATH, 'all_statistics_trees_k1.pkl'))\n",
    "statistics_trees_k1.head()\n",
    "# statistics_trees_k1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_trees_k2 = pd.read_pickle(os.path.join(\n",
    "    BASE_PATH, 'all_statistics_trees_k2to5.pkl'))\n",
    "statistics_trees_k2['dtc_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_acc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_acc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['dtc_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_auroc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_auroc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['dtc_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.dtc_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['rfc_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.rfc_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "statistics_trees_k2['gbt_auprc_agg'] = [np.dot(statistics_trees_k2.n_per_k.to_numpy()[i], statistics_trees_k2.gbt_auprc.to_numpy(\n",
    ")[i])/sum(statistics_trees_k2.n_per_k.to_numpy()[i]) for i in range(0, len(statistics_trees_k2.n_per_k.to_numpy()))]\n",
    "\n",
    "# statistics_trees_k2['rfc_acc_agg'].isnull().sum()\n",
    "\n",
    "# post processing\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_acc_agg']] = 0.8568376068376068\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_acc_agg']] = 0.8910256410256411\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_acc_agg']] = 0.9220183486238532\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_auroc_agg']] = 0.735998794040322\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_auroc_agg']] = 0.9531877968578725\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_auroc_agg']] = 0.9220183486238532\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['dtc_auprc_agg']] = 0.735998794040322\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['rfc_auprc_agg']] = 0.9531877968578725\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt30', ['gbt_auprc_agg']] = 0.9688447899695461\n",
    "\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['dtc_acc']] = 0.8008849557522124\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['rfc_acc']] = 0.834070796460177\n",
    "# statistics_trees_k2.loc[statistics_trees_k2['id']=='Apt59', ['gbt_acc']] = 0.8561946902654868\n",
    "\n",
    "statistics_trees_k2.head(5)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing\n",
    "best_result = statistics_trees_k1.join(\n",
    "    statistics_trees_k2, lsuffix='_1', rsuffix='_2')\n",
    "\n",
    "best_result[\"max_dtc_acc\"] = best_result[[\n",
    "    \"dtc_acc_1\", \"dtc_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_acc\"] = best_result[[\n",
    "    \"rfc_acc_1\", \"rfc_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_acc\"] = best_result[[\n",
    "    \"gbt_acc_1\", \"gbt_acc_agg\"]].max(axis=1)\n",
    "best_result[\"max_dtc_auprc\"] = best_result[[\n",
    "    \"dtc_auprc_1\", \"dtc_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_auprc\"] = best_result[[\n",
    "    \"rfc_auprc_1\", \"rfc_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_auprc\"] = best_result[[\n",
    "    \"gbt_auprc_1\", \"gbt_auprc_agg\"]].max(axis=1)\n",
    "best_result[\"max_dtc_auroc\"] = best_result[[\n",
    "    \"dtc_auroc_1\", \"dtc_auroc_agg\"]].max(axis=1)\n",
    "best_result[\"max_rfc_auroc\"] = best_result[[\n",
    "    \"rfc_auroc_1\", \"rfc_auroc_agg\"]].max(axis=1)\n",
    "best_result[\"max_gbt_auroc\"] = best_result[[\n",
    "    \"gbt_auroc_1\", \"gbt_auroc_agg\"]].max(axis=1)\n",
    "\n",
    "\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['dtc_acc']] = 0.8325688073394495\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['rfc_acc']] = 0.8738532110091743\n",
    "#best_result.loc[best_result['id_2']=='Apt30', ['gbt_acc']] = 0.9220183486238532\n",
    "\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['dtc_acc']] = 0.8008849557522124\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['rfc_acc']] = 0.834070796460177\n",
    "#best_result.loc[best_result['id_2']=='Apt59', ['gbt_acc']] = 0.8561946902654868\n",
    "\n",
    "# best_result.describe()\n",
    "pd.set_option('display.max_columns', None)\n",
    "# best_result.head(10)\n",
    "# best_result['max_dtc_acc'].isnull().values.any()\n",
    "# best_result['rfc_acc'].isnull().sum()\n",
    "#index = best_result['rfc_acc'].index[best_result['rfc_acc'].apply(np.isnan)]\n",
    "FINAL = best_result[[\"max_dtc_acc\", \"max_dtc_auroc\", \"max_dtc_auprc\",\n",
    "                     \"max_rfc_acc\", \"max_rfc_auroc\", \"max_rfc_auprc\",\n",
    "                     \"max_gbt_acc\", \"max_gbt_auroc\", \"max_gbt_auprc\"]]\n",
    "FINAL.head()\n",
    "FINAL.describe()\n",
    "# print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector24 = pd.read_pickle(os.path.join(\n",
    "    BASE_PATH, 'all_statistics_trees_k1_24vevtor.pkl'))\n",
    "vector24.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "## **Useful Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe().show()\n",
    "# .printSchema()\n",
    "# .collect()\n",
    "# .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "## **Old**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "# #create json data from stored dataframe\n",
    "\n",
    "# def to_json(final):\n",
    "#     PERIOD=60\n",
    "\n",
    "#     data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "#     def date_to_str(o):\n",
    "#         if isinstance(o, datetime.datetime):\n",
    "#             return o.__str__()\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "#     L  = []\n",
    "\n",
    "#     import json\n",
    "#     import datetime\n",
    "#     import time\n",
    "\n",
    "#     r, c = data_for_json.shape\n",
    "#     for i in range(0, r):\n",
    "#         for j in range(0, c):\n",
    "#             data = {}\n",
    "#             data['id'] = data_for_json.columns.values[j]\n",
    "#             data['power'] = data_for_json.iloc[i][j]\n",
    "#             data['date']=data_for_json.index.tolist()[i]\n",
    "#             json_data = json.dumps(data,default=date_to_str)\n",
    "#             L.append(json_data)\n",
    "#             #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "#     json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "#     return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load by schema\n",
    "\n",
    "# schema = StructType([\n",
    "#   StructField(\"num\", IntegerType()),\n",
    "#     StructField(\"date\", TimestampType()),\n",
    "#     StructField(\"id\", StringType()),\n",
    "#   StructField(\"power\", ArrayType(\n",
    "#       StructType([\n",
    "#           StructField(\"H0\", FloatType(), True),\n",
    "#           StructField(\"H1\", FloatType(), True),\n",
    "#           StructField(\"H2\", FloatType(), True),\n",
    "#           StructField(\"H3\", FloatType(), True),\n",
    "#           StructField(\"H4\", FloatType(), True),\n",
    "#           StructField(\"H5\", FloatType(), True),\n",
    "#           StructField(\"H6\", FloatType(), True),\n",
    "#           StructField(\"H7\", FloatType(), True),\n",
    "#           StructField(\"H8\", FloatType(), True),\n",
    "#           StructField(\"H9\", FloatType(), True),\n",
    "#           StructField(\"H10\", FloatType(), True),\n",
    "#           StructField(\"H11\", FloatType(), True),\n",
    "#           StructField(\"H12\", FloatType(), True),\n",
    "#           StructField(\"H13\", FloatType(), True),\n",
    "#           StructField(\"H14\", FloatType(), True),\n",
    "#           StructField(\"H15\", FloatType(), True),\n",
    "#           StructField(\"H16\", FloatType(), True),\n",
    "#           StructField(\"H17\", FloatType(), True),\n",
    "#           StructField(\"H18\", FloatType(), True),\n",
    "#           StructField(\"H19\", FloatType(), True),\n",
    "#           StructField(\"H20\", FloatType(), True),\n",
    "#           StructField(\"H21\", FloatType(), True),\n",
    "#           StructField(\"H22\", FloatType(), True),\n",
    "#           StructField(\"H23\", FloatType(), True)\n",
    "#       ])\n",
    "#    )\n",
    "#              )])\n",
    "\n",
    "# a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "# a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf\n",
    "\n",
    "# my_schema = StructType([\n",
    "#     StructField(\"id\", IntegerType()),\n",
    "#     StructField(\"age\", IntegerType())])\n",
    "# df=spark.read.csv(\"test.csv\", header=True,schema=my_schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# def plus_one(a):\n",
    "#     return a+1\n",
    "\n",
    "# plus_one_udf = pandas_udf(plus_one, returnType=IntegerType())\n",
    "\n",
    "# df.select(plus_one_udf(col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test pandas_udf (for array input)\n",
    "\n",
    "# df = spark.createDataFrame([([1,2,3,4,5,6],'val1'),([4,5,6,7,8,9],'val2')],['col1','col2'])\n",
    "# df.show()\n",
    "\n",
    "# @pandas_udf(ArrayType(LongType()))\n",
    "# def func(v):\n",
    "#     res=[]\n",
    "#     for row in v:\n",
    "#         temp=[]\n",
    "#         for k in range(len(row)):\n",
    "#             if (k<=2) or (k>4):\n",
    "#                 temp.append(row[k])\n",
    "#             else:\n",
    "#                 temp.append(row[k]*0)\n",
    "#         res.append(temp)\n",
    "#     return pd.Series(res)\n",
    "\n",
    "# df.withColumn('col3',func(df.col1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "377px",
    "left": "23px",
    "top": "138px",
    "width": "239.35px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
