{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import col, split\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "DATASET_PATH='/Users/Soroush/Desktop/Thesis/Code/dataset/'\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Install spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfe7HgRpv5kg"
   },
   "outputs": [],
   "source": [
    "# Install spark and dependencies\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://mirror.its.dal.ca/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz #or any other version\n",
    "!tar xvf spark-2.4.5-bin-hadoop2.7.tgz #based on version\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set up required environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\" #based on version\n",
    "\n",
    "#imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "#test\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "file_loc = './sample_data/california_housing_train.csv'\n",
    "df = spark.read.csv(file_loc, inferSchema=True, header =True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "#load and save .read_pickle() and .to_pickle() \n",
    "\n",
    "#save\n",
    "\n",
    "#dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "#aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "#dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "#aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "#json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "#aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "\n",
    "#load\n",
    "\n",
    "dataset=pd.read_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "json_dataset=pd.read_pickle(DATASET_PATH+\"json_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1585581700927,
     "user": {
      "displayName": "Soroush Omidvar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjRgg64DA6c5195gG8XeYVCDkEbhnmqzeG9RSnqaQ=s64",
      "userId": "04227658996805655790"
     },
     "user_tz": -270
    },
    "id": "cmQMS3_4zUbv",
    "outputId": "6b49e8db-9c01-48e3-8a21-b4b1d3b27155"
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "# dataset address: http://traces.cs.umass.edu/index.php/Smart/Smart\n",
    "\n",
    "#Extract File\n",
    "#import tarfile\n",
    "#!tar -xf '/gdrive/My Drive/a.gzip' -C '/gdrive/My Drive/'\n",
    "\n",
    "#load\n",
    "\n",
    "def load_smart_star_dataset(dataset_path): \n",
    "    \n",
    "    #length of file path\n",
    "    LENGTH=len(dataset_path)+5\n",
    "\n",
    "    df_merged = pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    # 2014\n",
    "    #path_2014 = r'/gdrive/My Drive/Dataset/apartment/2014'\n",
    "    path_2014= dataset_path+'2014'\n",
    "    all_2014_paths = glob.glob(path_2014 + \"/Apt*.csv\")\n",
    "    df_merged_2014 = pd.DataFrame(columns=['date'])\n",
    "    for file_name in all_2014_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2014/\", \"\").replace(\"_2014.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2014 = pd.merge(df_merged_2014, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2015\n",
    "    df_merged_2015 = pd.DataFrame(columns=['date'])\n",
    "    #path_2015 = r'/gdrive/My Drive/Dataset/apartment/2015'\n",
    "    path_2015= dataset_path+'2015'\n",
    "    all_2015_paths = glob.glob(path_2015 + \"/Apt*.csv\")\n",
    "    for file_name in all_2015_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2015/\", \"\").replace(\"_2015.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2015 = pd.merge(df_merged_2015, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2016\n",
    "    df_merged_2016 = pd.DataFrame(columns=['date'])\n",
    "    #path_2016 = r'/gdrive/My Drive/Dataset/apartment/2016'\n",
    "    path_2016= dataset_path+'2016'\n",
    "    all_2016_paths = glob.glob(path_2016 + \"/Apt*.csv\")\n",
    "    for file_name in all_2016_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2016/\", \"\").replace(\"_2016.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2016 = pd.merge(df_merged_2016, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # merge all years\n",
    "    df_merged = df_merged_2014.append(df_merged_2015, ignore_index=True).append(df_merged_2016, ignore_index=True)\n",
    "    final = df_merged\n",
    "    #save\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPCWDW60rgQT"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkkOLNk0a04X"
   },
   "outputs": [],
   "source": [
    "#aggregate data\n",
    "\n",
    "def agg_by_date(final):\n",
    "    final['date'] = pd.to_datetime(final['date'],)\n",
    "    final.index=final['date']\n",
    "\n",
    "    final_agg_by_hour = final.resample('60T').sum(min_count=1)  #for more than to NaN : .apply(lambda x: x.sum() if x.isnull().sum() <= 2 else np.nan)\n",
    "\n",
    "    final = final_agg_by_hour.resample('D').aggregate(lambda x: x.tolist()) # for tuple: .aggregate(lambda x: tuple(x))\n",
    "\n",
    "    #remove first and last row\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2016-12-28'),inplace=True)\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2014-10-15'),inplace=True)\n",
    "    #final=final.iloc[1:-1]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "#create json data from stored dataframe\n",
    "\n",
    "def to_json(final):\n",
    "    PERIOD=60\n",
    "\n",
    "    data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "    def date_to_str(o):\n",
    "        if isinstance(o, datetime.datetime):\n",
    "            return o.__str__()\n",
    "\n",
    "    json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "    L  = []\n",
    "\n",
    "    import json\n",
    "    import datetime\n",
    "    import time\n",
    "\n",
    "    r, c = data_for_json.shape\n",
    "    for i in range(0, r):\n",
    "        for j in range(0, c):\n",
    "            data = {}\n",
    "            data['id'] = data_for_json.columns.values[j]\n",
    "            data['power'] = data_for_json.iloc[i][j]\n",
    "            data['date']=data_for_json.index.tolist()[i]\n",
    "            json_data = json.dumps(data,default=date_to_str)\n",
    "            L.append(json_data)\n",
    "            #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "    json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "    return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column_based df to row_based\n",
    "\n",
    "def col_to_row(df):\n",
    "    #house_id=df.columns\n",
    "    df.reset_index(inplace=True)\n",
    "    return pd.melt(df, id_vars=['date'],value_name='power')\n",
    "\n",
    "#aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#aggregated_dataset_rowBased=col_to_row(aggregated_dataset)\n",
    "#aggregated_dataset_rowBased.sort_values(['date', 'variable'], ascending=[True, True],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- variable: string (nullable = true)\n",
      " |-- power: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"anomaly_detection\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "#define schema\n",
    "schema = StructType([\n",
    "    StructField(\"num\", IntegerType()),\n",
    "    StructField(\"date\", TimestampType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"power\", StringType())])\n",
    "\n",
    "#read data\n",
    "sdf = spark.read.format('csv').options(header='true', schema=schema).load(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#convert power to array\n",
    "def string_power_to_array(sdf):\n",
    "    temp=sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"),\"\\\\]\",\"\")\n",
    "                        .alias(\"power\"))\n",
    "    temp=temp.withColumn(\"power\",split(col(\"power\"), \",\\s*\")\n",
    "                        .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "sdf=string_power_to_array(sdf)\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def string_to_list(t: str):\n",
    "    return t.strip('][').split(', ') \n",
    "\n",
    "\n",
    "udf_parse_json = udf(lambda str: string_to_list(str), json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc_QASHrxmQ2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataset = load_smart_star_dataset(DATASET_PATH)\n",
    "\n",
    "#aggregate by date\n",
    "aggregated_dataset=agg_by_date(dataset)\n",
    "aggregated_dataset=aggregated_dataset[1:-1]\n",
    "\n",
    "#to json\n",
    "json_dataset=to_json(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- variable: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sdf.printSchema()\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "#cast\n",
    "def string_to_list(t: str):\n",
    "    return t.strip('][').split(', ') \n",
    "     \n",
    "\n",
    "sdf.withColumn(\"greetings\", string_to_list(col(\"power\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'CSV data source does not support array<struct<H0:float,H1:float,H2:float,H3:float,H4:float,H5:float,H6:float,H7:float,H8:float,H9:float,H10:float,H11:float,H12:float,H13:float,H14:float,H15:float,H16:float,H17:float,H18:float,H19:float,H20:float,H21:float,H22:float,H23:float>> data type.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o952.load.\n: org.apache.spark.sql.AnalysisException: CSV data source does not support array<struct<H0:float,H1:float,H2:float,H3:float,H4:float,H5:float,H6:float,H7:float,H8:float,H9:float,H10:float,H11:float,H12:float,H13:float,H14:float,H15:float,H16:float,H17:float,H18:float,H19:float,H20:float,H21:float,H22:float,H23:float>> data type.;\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:67)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifyReadSchema(DataSourceUtils.scala:41)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:400)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4ba341a4d996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m              )])\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"f.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'CSV data source does not support array<struct<H0:float,H1:float,H2:float,H3:float,H4:float,H5:float,H6:float,H7:float,H8:float,H9:float,H10:float,H11:float,H12:float,H13:float,H14:float,H15:float,H16:float,H17:float,H18:float,H19:float,H20:float,H21:float,H22:float,H23:float>> data type.;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"num\", IntegerType()),\n",
    "    StructField(\"date\", TimestampType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "  StructField(\"power\", ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"H0\", FloatType(), True),\n",
    "          StructField(\"H1\", FloatType(), True),\n",
    "          StructField(\"H2\", FloatType(), True),\n",
    "          StructField(\"H3\", FloatType(), True),\n",
    "          StructField(\"H4\", FloatType(), True),\n",
    "          StructField(\"H5\", FloatType(), True),\n",
    "          StructField(\"H6\", FloatType(), True),\n",
    "          StructField(\"H7\", FloatType(), True),\n",
    "          StructField(\"H8\", FloatType(), True),\n",
    "          StructField(\"H9\", FloatType(), True),\n",
    "          StructField(\"H10\", FloatType(), True),\n",
    "          StructField(\"H11\", FloatType(), True),\n",
    "          StructField(\"H12\", FloatType(), True),\n",
    "          StructField(\"H13\", FloatType(), True),\n",
    "          StructField(\"H14\", FloatType(), True),\n",
    "          StructField(\"H15\", FloatType(), True),\n",
    "          StructField(\"H16\", FloatType(), True),\n",
    "          StructField(\"H17\", FloatType(), True),\n",
    "          StructField(\"H18\", FloatType(), True),\n",
    "          StructField(\"H19\", FloatType(), True),\n",
    "          StructField(\"H20\", FloatType(), True),\n",
    "          StructField(\"H21\", FloatType(), True),\n",
    "          StructField(\"H22\", FloatType(), True),\n",
    "          StructField(\"H23\", FloatType(), True)\n",
    "      ])\n",
    "   )\n",
    "             )])\n",
    "\n",
    "a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------+--------------------+\n",
      "|  _c0|               date|variable|               power|\n",
      "+-----+-------------------+--------+--------------------+\n",
      "|15624|2014-08-14 00:00:00|    Apt1|[0.0, 0.0, 0.0, 0...|\n",
      "|85932|2014-08-14 00:00:00|   Apt10|[0.0, 0.0, 0.0, 0...|\n",
      "|78120|2014-08-14 00:00:00|  Apt100|[0.0, 0.0, 0.0, 0...|\n",
      "|78988|2014-08-14 00:00:00|  Apt101|[0.0, 0.0, 0.0, 0...|\n",
      "|37324|2014-08-14 00:00:00|  Apt102|[0.985781111, 0.5...|\n",
      "|36456|2014-08-14 00:00:00|  Apt103|[0.0, 0.0, 0.0, 0...|\n",
      "| 3472|2014-08-14 00:00:00|  Apt104|[0.0, 0.0, 0.0, 0...|\n",
      "| 4340|2014-08-14 00:00:00|  Apt105|[1.42481, 2.96944...|\n",
      "|68572|2014-08-14 00:00:00|  Apt106|[0.00015000000000...|\n",
      "|67704|2014-08-14 00:00:00|  Apt107|[0.0, 0.0, 0.0, 0...|\n",
      "|42532|2014-08-14 00:00:00|  Apt108|[0.0, 0.0, 0.0, 0...|\n",
      "|39928|2014-08-14 00:00:00|  Apt109|[0.0, 0.0, 0.0, 0...|\n",
      "|85064|2014-08-14 00:00:00|   Apt11|[0.0, 0.0, 0.0, 0...|\n",
      "|52948|2014-08-14 00:00:00|  Apt110|[0.0, 0.0, 0.0, 0...|\n",
      "|53816|2014-08-14 00:00:00|  Apt111|[2.521871112, 3.3...|\n",
      "|93744|2014-08-14 00:00:00|  Apt112|[nan, nan, nan, n...|\n",
      "|11284|2014-08-14 00:00:00|  Apt113|[0.0, 0.0, 0.0, 0...|\n",
      "|21700|2014-08-14 00:00:00|  Apt114|[1.87641777799999...|\n",
      "|27776|2014-08-14 00:00:00|   Apt12|[0.0, 0.0, 0.0, 0...|\n",
      "|28644|2014-08-14 00:00:00|   Apt13|[0.0, 0.0, 0.0, 0...|\n",
      "+-----+-------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sc = SparkContext.getOrCreate()\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"anomaly_detection\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "#load dataset\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema='true').load(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#cast\n",
    "def string_to_list(t):\n",
    "    return t.strip('][').split(', ') \n",
    "\n",
    "sdf.show()\n",
    "#sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = SparkSession.builder.appName(\"StackOverFlowSurvey\").getOrCreate()\n",
    "type(house_id)\n",
    "res = {house_id[i]: 'int' for i in range(len(house_id))} \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_dataset_rowBased=col_to_row(aggregated_dataset)\n",
    "aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(*(col(c).cast(\"float\").alias(c) for c in house_id))\n",
    "sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=aggregated_dataset.iloc[1].to_frame()\n",
    "test=test.reset_index()\n",
    "test.columns=['id','features']\n",
    "test.head()\n",
    "\n",
    "test['features'].values.tolist()\n",
    "\n",
    "temp = pd.DataFrame(test['features'].values.tolist(), columns=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23'])\n",
    "temp.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
