{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "DATASET_PATH='/Users/Soroush/Desktop/Thesis/Code/dataset/'\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Install spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfe7HgRpv5kg"
   },
   "outputs": [],
   "source": [
    "# Install spark and dependencies\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://mirror.its.dal.ca/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz #or any other version\n",
    "!tar xvf spark-2.4.5-bin-hadoop2.7.tgz #based on version\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set up required environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\" #based on version\n",
    "\n",
    "#imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "#test\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "file_loc = './sample_data/california_housing_train.csv'\n",
    "df = spark.read.csv(file_loc, inferSchema=True, header =True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "#load and save .read_pickle() and .to_pickle() \n",
    "\n",
    "#save\n",
    "#dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "#aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "\n",
    "#load\n",
    "#dataset=pd.read_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "#aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#json_dataset=pd.read_pickle(DATASET_PATH+\"json_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1585581700927,
     "user": {
      "displayName": "Soroush Omidvar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjRgg64DA6c5195gG8XeYVCDkEbhnmqzeG9RSnqaQ=s64",
      "userId": "04227658996805655790"
     },
     "user_tz": -270
    },
    "id": "cmQMS3_4zUbv",
    "outputId": "6b49e8db-9c01-48e3-8a21-b4b1d3b27155"
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "# dataset address: http://traces.cs.umass.edu/index.php/Smart/Smart\n",
    "\n",
    "#Extract File\n",
    "#import tarfile\n",
    "#!tar -xf '/gdrive/My Drive/a.gzip' -C '/gdrive/My Drive/'\n",
    "\n",
    "#load\n",
    "\n",
    "def load_smart_star_dataset(dataset_path): \n",
    "    \n",
    "    #length of file path\n",
    "    LENGTH=len(dataset_path)+5\n",
    "\n",
    "    df_merged = pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    # 2014\n",
    "    #path_2014 = r'/gdrive/My Drive/Dataset/apartment/2014'\n",
    "    path_2014= dataset_path+'2014'\n",
    "    all_2014_paths = glob.glob(path_2014 + \"/Apt*.csv\")\n",
    "    df_merged_2014 = pd.DataFrame(columns=['date'])\n",
    "    for file_name in all_2014_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2014/\", \"\").replace(\"_2014.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2014 = pd.merge(df_merged_2014, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2015\n",
    "    df_merged_2015 = pd.DataFrame(columns=['date'])\n",
    "    #path_2015 = r'/gdrive/My Drive/Dataset/apartment/2015'\n",
    "    path_2015= dataset_path+'2015'\n",
    "    all_2015_paths = glob.glob(path_2015 + \"/Apt*.csv\")\n",
    "    for file_name in all_2015_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2015/\", \"\").replace(\"_2015.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2015 = pd.merge(df_merged_2015, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2016\n",
    "    df_merged_2016 = pd.DataFrame(columns=['date'])\n",
    "    #path_2016 = r'/gdrive/My Drive/Dataset/apartment/2016'\n",
    "    path_2016= dataset_path+'2016'\n",
    "    all_2016_paths = glob.glob(path_2016 + \"/Apt*.csv\")\n",
    "    for file_name in all_2016_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2016/\", \"\").replace(\"_2016.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2016 = pd.merge(df_merged_2016, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # merge all years\n",
    "    df_merged = df_merged_2014.append(df_merged_2015, ignore_index=True).append(df_merged_2016, ignore_index=True)\n",
    "    final = df_merged\n",
    "    #save\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPCWDW60rgQT"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkkOLNk0a04X"
   },
   "outputs": [],
   "source": [
    "#aggregate data\n",
    "\n",
    "def agg_by_date(final):\n",
    "    final['date'] = pd.to_datetime(final['date'],)\n",
    "    final.index=final['date']\n",
    "\n",
    "    final_agg_by_hour = final.resample('60T').sum(min_count=1)  #for more than to NaN : .apply(lambda x: x.sum() if x.isnull().sum() <= 2 else np.nan)\n",
    "\n",
    "    final = final_agg_by_hour.resample('D').aggregate(lambda x: x.tolist()) # for tuple: .aggregate(lambda x: tuple(x))\n",
    "\n",
    "    #remove first and last row\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2016-12-28'),inplace=True)\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2014-10-15'),inplace=True)\n",
    "    #final=final.iloc[1:-1]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "#create json data from stored dataframe\n",
    "\n",
    "def to_json(final):\n",
    "    PERIOD=60\n",
    "\n",
    "    data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "    def date_to_str(o):\n",
    "        if isinstance(o, datetime.datetime):\n",
    "            return o.__str__()\n",
    "\n",
    "    json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "    L  = []\n",
    "\n",
    "    import json\n",
    "    import datetime\n",
    "    import time\n",
    "\n",
    "    r, c = data_for_json.shape\n",
    "    for i in range(0, r):\n",
    "        for j in range(0, c):\n",
    "            data = {}\n",
    "            data['id'] = data_for_json.columns.values[j]\n",
    "            data['power'] = data_for_json.iloc[i][j]\n",
    "            data['date']=data_for_json.index.tolist()[i]\n",
    "            json_data = json.dumps(data,default=date_to_str)\n",
    "            L.append(json_data)\n",
    "            #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "    json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "    return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column_based df to row_based\n",
    "\n",
    "def col_to_row(df):\n",
    "    #house_id=df.columns\n",
    "    df.reset_index(inplace=True)\n",
    "    return pd.melt(df, id_vars=['date'],value_name='power')\n",
    "\n",
    "#aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#aggregated_dataset_rowBased=col_to_row(aggregated_dataset)\n",
    "#aggregated_dataset_rowBased.sort_values(['date', 'variable'], ascending=[True, True],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc_QASHrxmQ2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataset = load_smart_star_dataset(DATASET_PATH)\n",
    "\n",
    "#aggregate by date\n",
    "aggregated_dataset=agg_by_date(dataset)\n",
    "\n",
    "#to json\n",
    "json_dataset=to_json(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15624</th>\n",
       "      <td>2014-08-14</td>\n",
       "      <td>Apt1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85932</th>\n",
       "      <td>2014-08-14</td>\n",
       "      <td>Apt10</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78120</th>\n",
       "      <td>2014-08-14</td>\n",
       "      <td>Apt100</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78988</th>\n",
       "      <td>2014-08-14</td>\n",
       "      <td>Apt101</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37324</th>\n",
       "      <td>2014-08-14</td>\n",
       "      <td>Apt102</td>\n",
       "      <td>[0.985781111, 0.5892933340000001, 0.5635333330...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date variable                                              power\n",
       "15624 2014-08-14     Apt1           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "85932 2014-08-14    Apt10           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "78120 2014-08-14   Apt100           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "78988 2014-08-14   Apt101           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "37324 2014-08-14   Apt102  [0.985781111, 0.5892933340000001, 0.5635333330..."
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------+--------------------+\n",
      "|  _c0|               date|variable|               power|\n",
      "+-----+-------------------+--------+--------------------+\n",
      "|15624|2014-08-14 00:00:00|    Apt1|[0.0, 0.0, 0.0, 0...|\n",
      "|85932|2014-08-14 00:00:00|   Apt10|[0.0, 0.0, 0.0, 0...|\n",
      "|78120|2014-08-14 00:00:00|  Apt100|[0.0, 0.0, 0.0, 0...|\n",
      "|78988|2014-08-14 00:00:00|  Apt101|[0.0, 0.0, 0.0, 0...|\n",
      "|37324|2014-08-14 00:00:00|  Apt102|[0.985781111, 0.5...|\n",
      "|36456|2014-08-14 00:00:00|  Apt103|[0.0, 0.0, 0.0, 0...|\n",
      "| 3472|2014-08-14 00:00:00|  Apt104|[0.0, 0.0, 0.0, 0...|\n",
      "| 4340|2014-08-14 00:00:00|  Apt105|[1.42481, 2.96944...|\n",
      "|68572|2014-08-14 00:00:00|  Apt106|[0.00015000000000...|\n",
      "|67704|2014-08-14 00:00:00|  Apt107|[0.0, 0.0, 0.0, 0...|\n",
      "|42532|2014-08-14 00:00:00|  Apt108|[0.0, 0.0, 0.0, 0...|\n",
      "|39928|2014-08-14 00:00:00|  Apt109|[0.0, 0.0, 0.0, 0...|\n",
      "|85064|2014-08-14 00:00:00|   Apt11|[0.0, 0.0, 0.0, 0...|\n",
      "|52948|2014-08-14 00:00:00|  Apt110|[0.0, 0.0, 0.0, 0...|\n",
      "|53816|2014-08-14 00:00:00|  Apt111|[2.521871112, 3.3...|\n",
      "|93744|2014-08-14 00:00:00|  Apt112|[nan, nan, nan, n...|\n",
      "|11284|2014-08-14 00:00:00|  Apt113|[0.0, 0.0, 0.0, 0...|\n",
      "|21700|2014-08-14 00:00:00|  Apt114|[1.87641777799999...|\n",
      "|27776|2014-08-14 00:00:00|   Apt12|[0.0, 0.0, 0.0, 0...|\n",
      "|28644|2014-08-14 00:00:00|   Apt13|[0.0, 0.0, 0.0, 0...|\n",
      "+-----+-------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#load dataset\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema='true').load(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#cast\n",
    "def string_to_list(t):\n",
    "    return t.strip('][').split(', ') \n",
    "\n",
    "sdf.show()\n",
    "#sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = SparkSession.builder.appName(\"StackOverFlowSurvey\").getOrCreate()\n",
    "type(house_id)\n",
    "res = {house_id[i]: 'int' for i in range(len(house_id))} \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(*(col(c).cast(\"float\").alias(c) for c in house_id))\n",
    "sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=aggregated_dataset.iloc[1].to_frame()\n",
    "test=test.reset_index()\n",
    "test.columns=['id','features']\n",
    "test.head()\n",
    "\n",
    "test['features'].values.tolist()\n",
    "\n",
    "temp = pd.DataFrame(test['features'].values.tolist(), columns=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23'])\n",
    "temp.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
