{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqqSvu5q0ft_"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import col, split, size, isnan, array_contains, array_min, when, count\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, FloatType\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import glob\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "DATASET_PATH='/Users/Soroush/Desktop/Thesis/Code/dataset/'\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vge79PtGqKo8"
   },
   "source": [
    "# **Install spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfe7HgRpv5kg"
   },
   "outputs": [],
   "source": [
    "# Install spark and dependencies\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://mirror.its.dal.ca/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz #or any other version\n",
    "!tar xvf spark-2.4.5-bin-hadoop2.7.tgz #based on version\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set up required environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\" #based on version\n",
    "\n",
    "#imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "#test\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "file_loc = './sample_data/california_housing_train.csv'\n",
    "df = spark.read.csv(file_loc, inferSchema=True, header =True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DiwZdhIqSyU"
   },
   "source": [
    "# **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTz4gxgr6JqK"
   },
   "outputs": [],
   "source": [
    "#load and save .read_pickle() and .to_pickle() \n",
    "\n",
    "#save\n",
    "\n",
    "#dataset.to_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "#aggregated_dataset.to_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#json_dataset.to_pickle(DATASET_PATH+\"json_dataset.pkl\")\n",
    "#dataset.to_csv(DATASET_PATH+\"dataset.csv\")\n",
    "#aggregated_dataset.to_csv(DATASET_PATH+\"aggregated_dataset.csv\")\n",
    "#json_dataset.to_csv(DATASET_PATH+\"json_dataset.csv\")\n",
    "#aggregated_dataset_rowBased.to_csv(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "\n",
    "#load\n",
    "\n",
    "dataset=pd.read_pickle(DATASET_PATH+\"dataset.pkl\")\n",
    "aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "json_dataset=pd.read_pickle(DATASET_PATH+\"json_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1585581700927,
     "user": {
      "displayName": "Soroush Omidvar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjRgg64DA6c5195gG8XeYVCDkEbhnmqzeG9RSnqaQ=s64",
      "userId": "04227658996805655790"
     },
     "user_tz": -270
    },
    "id": "cmQMS3_4zUbv",
    "outputId": "6b49e8db-9c01-48e3-8a21-b4b1d3b27155"
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "# dataset address: http://traces.cs.umass.edu/index.php/Smart/Smart\n",
    "\n",
    "#Extract File\n",
    "#import tarfile\n",
    "#!tar -xf '/gdrive/My Drive/a.gzip' -C '/gdrive/My Drive/'\n",
    "\n",
    "#load\n",
    "\n",
    "def load_smart_star_dataset(dataset_path): \n",
    "    \n",
    "    #length of file path\n",
    "    LENGTH=len(dataset_path)+5\n",
    "\n",
    "    df_merged = pd.DataFrame(columns=['date'])\n",
    "    \n",
    "    # 2014\n",
    "    #path_2014 = r'/gdrive/My Drive/Dataset/apartment/2014'\n",
    "    path_2014= dataset_path+'2014'\n",
    "    all_2014_paths = glob.glob(path_2014 + \"/Apt*.csv\")\n",
    "    df_merged_2014 = pd.DataFrame(columns=['date'])\n",
    "    for file_name in all_2014_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2014/\", \"\").replace(\"_2014.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2014 = pd.merge(df_merged_2014, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2015\n",
    "    df_merged_2015 = pd.DataFrame(columns=['date'])\n",
    "    #path_2015 = r'/gdrive/My Drive/Dataset/apartment/2015'\n",
    "    path_2015= dataset_path+'2015'\n",
    "    all_2015_paths = glob.glob(path_2015 + \"/Apt*.csv\")\n",
    "    for file_name in all_2015_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2015/\", \"\").replace(\"_2015.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2015 = pd.merge(df_merged_2015, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # 2016\n",
    "    df_merged_2016 = pd.DataFrame(columns=['date'])\n",
    "    #path_2016 = r'/gdrive/My Drive/Dataset/apartment/2016'\n",
    "    path_2016= dataset_path+'2016'\n",
    "    all_2016_paths = glob.glob(path_2016 + \"/Apt*.csv\")\n",
    "    for file_name in all_2016_paths:\n",
    "        column_name = file_name[LENGTH:-9]\n",
    "        # column_name = file_name.replace(\"dataset/2016/\", \"\").replace(\"_2016.csv\",\"\")\n",
    "        df = pd.read_csv(file_name, names=[\"date\", column_name])\n",
    "        df_merged_2016 = pd.merge(df_merged_2016, df, on='date', how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # merge all years\n",
    "    df_merged = df_merged_2014.append(df_merged_2015, ignore_index=True).append(df_merged_2016, ignore_index=True)\n",
    "    final = df_merged\n",
    "    #save\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPCWDW60rgQT"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkkOLNk0a04X"
   },
   "outputs": [],
   "source": [
    "#aggregate data\n",
    "\n",
    "def agg_by_date(final):\n",
    "    final['date'] = pd.to_datetime(final['date'],)\n",
    "    final.index=final['date']\n",
    "\n",
    "    final_agg_by_hour = final.resample('60T').sum(min_count=1)  #for more than to NaN : .apply(lambda x: x.sum() if x.isnull().sum() <= 2 else np.nan)\n",
    "\n",
    "    final = final_agg_by_hour.resample('D').aggregate(lambda x: x.tolist()) # for tuple: .aggregate(lambda x: tuple(x))\n",
    "\n",
    "    #remove first and last row\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2016-12-28'),inplace=True)\n",
    "    #aggregated_dataset.drop(pd.to_datetime('2014-10-15'),inplace=True)\n",
    "    #final=final.iloc[1:-1]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xLMPlUJW_cn"
   },
   "outputs": [],
   "source": [
    "#create json data from stored dataframe\n",
    "\n",
    "def to_json(final):\n",
    "    PERIOD=60\n",
    "\n",
    "    data_for_json=final.loc[:, final.columns != 'date']\n",
    "\n",
    "    def date_to_str(o):\n",
    "        if isinstance(o, datetime.datetime):\n",
    "            return o.__str__()\n",
    "\n",
    "    json_dataframe = pd.DataFrame(columns=['data']) #creates a new dataframe that's empty\n",
    "    L  = []\n",
    "\n",
    "    import json\n",
    "    import datetime\n",
    "    import time\n",
    "\n",
    "    r, c = data_for_json.shape\n",
    "    for i in range(0, r):\n",
    "        for j in range(0, c):\n",
    "            data = {}\n",
    "            data['id'] = data_for_json.columns.values[j]\n",
    "            data['power'] = data_for_json.iloc[i][j]\n",
    "            data['date']=data_for_json.index.tolist()[i]\n",
    "            json_data = json.dumps(data,default=date_to_str)\n",
    "            L.append(json_data)\n",
    "            #json_dataframe=json_dataframe.append(json_data,ignore_index=True)\n",
    "\n",
    "    json_dataframe = pd.DataFrame(L, columns=['data'])\n",
    "    return json_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column_based df to row_based\n",
    "\n",
    "def col_to_row(df):\n",
    "    #house_id=df.columns\n",
    "    df.reset_index(inplace=True)\n",
    "    return pd.melt(df, id_vars=['date'],value_name='power')\n",
    "\n",
    "#aggregated_dataset=pd.read_pickle(DATASET_PATH+\"aggregated_dataset.pkl\")\n",
    "#aggregated_dataset_rowBased=col_to_row(aggregated_dataset)\n",
    "#aggregated_dataset_rowBased.sort_values(['date', 'variable'], ascending=[True, True],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx4ORKJlZiHa"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9utJRS41p5X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+--------------------+\n",
      "|  #|               date|   id|               power|\n",
      "+---+-------------------+-----+--------------------+\n",
      "|  0|2014-08-15 00:00:00|Apt36|[1.96932, 0.38400...|\n",
      "|  1|2014-08-16 00:00:00|Apt36|[0.61410224, 0.44...|\n",
      "|  2|2014-08-17 00:00:00|Apt36|[1.3780422, 1.165...|\n",
      "|  3|2014-08-18 00:00:00|Apt36|[1.1634789, 2.013...|\n",
      "|  4|2014-08-19 00:00:00|Apt36|[1.1126077, 1.294...|\n",
      "|  5|2014-08-20 00:00:00|Apt36|[1.4072489, 1.076...|\n",
      "|  6|2014-08-21 00:00:00|Apt36|[1.4892989, 1.260...|\n",
      "|  7|2014-08-22 00:00:00|Apt36|[0.5883856, 0.554...|\n",
      "|  8|2014-08-23 00:00:00|Apt36|[0.52373445, 1.48...|\n",
      "|  9|2014-08-24 00:00:00|Apt36|[0.39280555, 1.30...|\n",
      "| 10|2014-08-25 00:00:00|Apt36|[0.4686211, 0.502...|\n",
      "| 11|2014-08-26 00:00:00|Apt36|[1.2439622, 0.637...|\n",
      "| 12|2014-08-27 00:00:00|Apt36|[1.8882623, 2.731...|\n",
      "| 13|2014-08-28 00:00:00|Apt36|[1.29602, 1.83715...|\n",
      "| 14|2014-08-29 00:00:00|Apt36|[1.3385545, 1.459...|\n",
      "| 15|2014-08-30 00:00:00|Apt36|[1.7708489, 3.387...|\n",
      "| 16|2014-08-31 00:00:00|Apt36|[1.1021422, 1.852...|\n",
      "| 17|2014-09-01 00:00:00|Apt36|[2.55456, 1.86436...|\n",
      "| 18|2014-09-02 00:00:00|Apt36|[0.40945, 1.46271...|\n",
      "| 19|2014-09-03 00:00:00|Apt36|[1.6452545, 2.654...|\n",
      "+---+-------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-------------------+-----+--------------------+-----+\n",
      "|  #|               date|   id|               power|    V|\n",
      "+---+-------------------+-----+--------------------+-----+\n",
      "|  0|2014-08-15 00:00:00|Apt36|[1.96932, 0.38400...| true|\n",
      "|  1|2014-08-16 00:00:00|Apt36|[0.61410224, 0.44...| true|\n",
      "|  2|2014-08-17 00:00:00|Apt36|[1.3780422, 1.165...| true|\n",
      "|  3|2014-08-18 00:00:00|Apt36|[1.1634789, 2.013...| true|\n",
      "|  4|2014-08-19 00:00:00|Apt36|[1.1126077, 1.294...| true|\n",
      "|  5|2014-08-20 00:00:00|Apt36|[1.4072489, 1.076...| true|\n",
      "|  6|2014-08-21 00:00:00|Apt36|[1.4892989, 1.260...| true|\n",
      "|  7|2014-08-22 00:00:00|Apt36|[0.5883856, 0.554...| true|\n",
      "|  8|2014-08-23 00:00:00|Apt36|[0.52373445, 1.48...| true|\n",
      "|  9|2014-08-24 00:00:00|Apt36|[0.39280555, 1.30...| true|\n",
      "| 10|2014-08-25 00:00:00|Apt36|[0.4686211, 0.502...| true|\n",
      "| 11|2014-08-26 00:00:00|Apt36|[1.2439622, 0.637...| true|\n",
      "| 12|2014-08-27 00:00:00|Apt36|[1.8882623, 2.731...| true|\n",
      "| 13|2014-08-28 00:00:00|Apt36|[1.29602, 1.83715...| true|\n",
      "| 14|2014-08-29 00:00:00|Apt36|[1.3385545, 1.459...| true|\n",
      "| 15|2014-08-30 00:00:00|Apt36|[1.7708489, 3.387...| true|\n",
      "| 16|2014-08-31 00:00:00|Apt36|[1.1021422, 1.852...| true|\n",
      "| 17|2014-09-01 00:00:00|Apt36|[2.55456, 1.86436...| true|\n",
      "| 18|2014-09-02 00:00:00|Apt36|[0.40945, 1.46271...| true|\n",
      "| 19|2014-09-03 00:00:00|Apt36|[1.6452545, 2.654...| true|\n",
      "| 20|2014-09-04 00:00:00|Apt36|[3.2723212, 2.451...| true|\n",
      "| 21|2014-09-05 00:00:00|Apt36|[1.2240756, 2.237...| true|\n",
      "| 22|2014-09-06 00:00:00|Apt36|[2.2258666, 2.329...| true|\n",
      "| 23|2014-09-07 00:00:00|Apt36|[4.1976557, 3.688...| true|\n",
      "| 24|2014-09-08 00:00:00|Apt36|[2.6905644, 2.807...| true|\n",
      "| 25|2014-09-09 00:00:00|Apt36|[5.9824624, 7.958...| true|\n",
      "| 26|2014-09-10 00:00:00|Apt36|[7.0522513, 4.969...| true|\n",
      "| 27|2014-09-11 00:00:00|Apt36|[2.6638355, 5.018...| true|\n",
      "| 28|2014-09-12 00:00:00|Apt36|[2.2770889, 3.511...| true|\n",
      "| 29|2014-09-13 00:00:00|Apt36|[5.0781646, 2.985...| true|\n",
      "| 30|2014-09-14 00:00:00|Apt36|[3.4998944, 5.125...| true|\n",
      "| 31|2014-09-15 00:00:00|Apt36|[7.333719, 5.4015...| true|\n",
      "| 32|2014-09-16 00:00:00|Apt36|[5.087322, 4.8021...| true|\n",
      "| 33|2014-09-17 00:00:00|Apt36|[3.5213778, 3.067...| true|\n",
      "| 34|2014-09-18 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 35|2014-09-19 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 36|2014-09-20 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 37|2014-09-21 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 38|2014-09-22 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 39|2014-09-23 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 40|2014-09-24 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 41|2014-09-25 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 42|2014-09-26 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 43|2014-09-27 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 44|2014-09-28 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 45|2014-09-29 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 46|2014-09-30 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 47|2014-10-01 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 48|2014-10-02 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 49|2014-10-03 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 50|2014-10-04 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 51|2014-10-05 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 52|2014-10-06 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 53|2014-10-07 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 54|2014-10-08 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 55|2014-10-09 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...| true|\n",
      "| 56|2014-10-10 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 57|2014-10-11 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 58|2014-10-12 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 59|2014-10-13 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 60|2014-10-14 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 61|2014-10-15 00:00:00|Apt36|[0.0, 0.0, 0.0, 0...|false|\n",
      "| 62|2014-10-16 00:00:00|Apt36|[0.0, 0.0, 0.0, 1...| true|\n",
      "| 63|2014-10-17 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 64|2014-10-18 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 65|2014-10-19 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 66|2014-10-20 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 67|2014-10-21 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 68|2014-10-22 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 69|2014-10-23 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 70|2014-10-24 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 71|2014-10-25 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 72|2014-10-26 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 73|2014-10-27 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 74|2014-10-28 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 75|2014-10-29 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 76|2014-10-30 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 77|2014-10-31 00:00:00|Apt36|[6.6668E-5, 6.666...| true|\n",
      "| 78|2014-11-01 00:00:00|Apt36|[1.33332E-4, 1.33...| true|\n",
      "| 79|2014-11-02 00:00:00|Apt36|[1.33332E-4, 2.66...| true|\n",
      "| 80|2014-11-03 00:00:00|Apt36|[1.33332E-4, 1.33...| true|\n",
      "| 81|2014-11-04 00:00:00|Apt36|[1.33332E-4, 1.33...| true|\n",
      "| 82|2014-11-05 00:00:00|Apt36|[1.33332E-4, 1.33...| true|\n",
      "| 83|2014-11-06 00:00:00|Apt36|[1.33332E-4, 1.33...| true|\n",
      "| 84|2014-11-07 00:00:00|Apt36|[2.0E-4, 2.0E-4, ...| true|\n",
      "| 85|2014-11-08 00:00:00|Apt36|[2.0E-4, 2.0E-4, ...| true|\n",
      "| 86|2014-11-09 00:00:00|Apt36|[2.66668E-4, 2.66...| true|\n",
      "| 87|2014-11-10 00:00:00|Apt36|[3.33332E-4, 3.33...| true|\n",
      "| 88|2014-11-11 00:00:00|Apt36|[3.3069167, 5.161...| true|\n",
      "| 89|2014-11-12 00:00:00|Apt36|[3.0284333, 3.719...| true|\n",
      "| 90|2014-11-13 00:00:00|Apt36|[3.8005645, 4.988...| true|\n",
      "| 91|2014-11-14 00:00:00|Apt36|[3.2806456, 3.463...| true|\n",
      "| 92|2014-11-15 00:00:00|Apt36|[5.0950947, 6.167...| true|\n",
      "| 93|2014-11-16 00:00:00|Apt36|[5.396663, 6.7414...| true|\n",
      "| 94|2014-11-17 00:00:00|Apt36|[5.005342, 4.3446...| true|\n",
      "| 95|2014-11-18 00:00:00|Apt36|[5.341809, 3.2732...| true|\n",
      "| 96|2014-11-19 00:00:00|Apt36|[5.9732676, 7.286...| true|\n",
      "| 97|2014-11-20 00:00:00|Apt36|[7.4603434, 5.827...| true|\n",
      "| 98|2014-11-21 00:00:00|Apt36|[5.934761, 5.8829...| true|\n",
      "| 99|2014-11-22 00:00:00|Apt36|[6.5312324, 7.011...| true|\n",
      "|100|2014-11-23 00:00:00|Apt36|[5.612279, 2.8327...| true|\n",
      "|101|2014-11-24 00:00:00|Apt36|[2.1587055, 1.369...| true|\n",
      "|102|2014-11-25 00:00:00|Apt36|[0.59770554, 0.32...| true|\n",
      "|103|2014-11-26 00:00:00|Apt36|[7.473913, 2.3080...| true|\n",
      "|104|2014-11-27 00:00:00|Apt36|[6.3958254, 4.672...| true|\n",
      "|105|2014-11-28 00:00:00|Apt36|[0.7734678, 5.343...| true|\n",
      "|106|2014-11-29 00:00:00|Apt36|[7.3865523, 4.456...| true|\n",
      "|107|2014-11-30 00:00:00|Apt36|[8.239565, 3.9254...| true|\n",
      "|108|2014-12-01 00:00:00|Apt36|[2.9575567, 2.395...| true|\n",
      "|109|2014-12-02 00:00:00|Apt36|[2.0174344, 2.081...| true|\n",
      "|110|2014-12-03 00:00:00|Apt36|[2.1589022, 2.093...| true|\n",
      "|111|2014-12-04 00:00:00|Apt36|[0.04887222, 0.28...| true|\n",
      "|112|2014-12-05 00:00:00|Apt36|[3.5432978, 2.678...| true|\n",
      "|113|2014-12-06 00:00:00|Apt36|[2.4412088, 1.310...| true|\n",
      "|114|2014-12-07 00:00:00|Apt36|[1.8246278, 0.249...| true|\n",
      "|115|2014-12-08 00:00:00|Apt36|[4.338169, 0.1767...| true|\n",
      "|116|2014-12-09 00:00:00|Apt36|[0.10050111, 0.18...| true|\n",
      "|117|2014-12-10 00:00:00|Apt36|[1.1344455, 0.092...| true|\n",
      "|118|2014-12-11 00:00:00|Apt36|[2.7198944, 0.117...| true|\n",
      "|119|2014-12-12 00:00:00|Apt36|[0.50169, 0.10361...| true|\n",
      "+---+-------------------+-----+--------------------+-----+\n",
      "only showing top 120 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"anomaly_detection\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "#define schema\n",
    "schema = StructType([\n",
    "    StructField(\"#\", IntegerType()),\n",
    "    StructField(\"date\", TimestampType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"power\", StringType())])\n",
    "\n",
    "#read data\n",
    "sdf = spark.read.format('csv').options(header='true',inferSchema=True ,schema=schema).load(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#rename columns\n",
    "def rename_dataframe(sdf):\n",
    "    names = ['#','date','id','power']\n",
    "    for c,n in zip(sdf.columns,names):\n",
    "        sdf=sdf.withColumnRenamed(c,n)\n",
    "    return sdf\n",
    "    \n",
    "sdf=rename_dataframe(sdf)\n",
    "    \n",
    "#convert power to array\n",
    "def string_power_to_array(sdf):\n",
    "    temp=sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"),\"\\\\]\",\"\")\n",
    "                        .alias(\"power\"))\n",
    "    temp=temp.withColumn(\"power\",split(col(\"power\"), \",\\s*\")\n",
    "                        .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "sdf=string_power_to_array(sdf)\n",
    "sdf.show()\n",
    "\n",
    "#add validation column\n",
    "def add_validation_column(sdf):\n",
    "    v=True\n",
    "    temp=sdf.withColumn(\"V\",f.when((size(sdf.power)==24) #array size\n",
    "                                   & ~(f.array_contains(sdf.power, float('nan'))) #containing NaN\n",
    "                                   & ~(f.array_max(sdf.power)<=0) #all 0\n",
    "                                   , True).otherwise(False))\n",
    "    return temp\n",
    "\n",
    "sdf=add_validation_column(sdf)\n",
    "sdf.show(120)\n",
    "\n",
    "def split_power(sdf):\n",
    "    temp=sdf.select(\"#\",\"date\",\"id\",\n",
    "           sdf.power[0].alias(\"H0\"), sdf.power[1].alias(\"H1\"), sdf.power[2].alias(\"H2\"),sdf.power[3].alias(\"H3\"),\n",
    "           sdf.power[4].alias(\"H4\"), sdf.power[5].alias(\"H5\"), sdf.power[6].alias(\"H6\"),sdf.power[7].alias(\"H7\"),\n",
    "           sdf.power[8].alias(\"H8\"), sdf.power[9].alias(\"H9\"), sdf.power[10].alias(\"H10\"),sdf.power[11].alias(\"H11\"),\n",
    "           sdf.power[12].alias(\"H12\"), sdf.power[13].alias(\"H13\"), sdf.power[14].alias(\"H14\"),sdf.power[15].alias(\"H15\"),\n",
    "           sdf.power[16].alias(\"H16\"), sdf.power[17].alias(\"H17\"), sdf.power[18].alias(\"H18\"),sdf.power[19].alias(\"H19\"),\n",
    "           sdf.power[20].alias(\"H20\"), sdf.power[21].alias(\"H21\"), sdf.power[22].alias(\"H22\"),sdf.power[23].alias(\"H23\"))\n",
    "    return temp\n",
    "    \n",
    "#sdf=split_power(sdf)\n",
    "#sdf.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc_QASHrxmQ2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataset = load_smart_star_dataset(DATASET_PATH)\n",
    "\n",
    "#aggregate by date\n",
    "aggregated_dataset=agg_by_date(dataset)\n",
    "aggregated_dataset=aggregated_dataset[1:-1]\n",
    "\n",
    "#to json\n",
    "json_dataset=to_json(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZey1SXJxkVl"
   },
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf.printSchema()\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "#cast\n",
    "def string_to_list(t: str):\n",
    "    return t.strip('][').split(', ') \n",
    "     \n",
    "\n",
    "sdf.withColumn(\"greetings\", string_to_list(col(\"power\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"num\", IntegerType()),\n",
    "    StructField(\"date\", TimestampType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "  StructField(\"power\", ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"H0\", FloatType(), True),\n",
    "          StructField(\"H1\", FloatType(), True),\n",
    "          StructField(\"H2\", FloatType(), True),\n",
    "          StructField(\"H3\", FloatType(), True),\n",
    "          StructField(\"H4\", FloatType(), True),\n",
    "          StructField(\"H5\", FloatType(), True),\n",
    "          StructField(\"H6\", FloatType(), True),\n",
    "          StructField(\"H7\", FloatType(), True),\n",
    "          StructField(\"H8\", FloatType(), True),\n",
    "          StructField(\"H9\", FloatType(), True),\n",
    "          StructField(\"H10\", FloatType(), True),\n",
    "          StructField(\"H11\", FloatType(), True),\n",
    "          StructField(\"H12\", FloatType(), True),\n",
    "          StructField(\"H13\", FloatType(), True),\n",
    "          StructField(\"H14\", FloatType(), True),\n",
    "          StructField(\"H15\", FloatType(), True),\n",
    "          StructField(\"H16\", FloatType(), True),\n",
    "          StructField(\"H17\", FloatType(), True),\n",
    "          StructField(\"H18\", FloatType(), True),\n",
    "          StructField(\"H19\", FloatType(), True),\n",
    "          StructField(\"H20\", FloatType(), True),\n",
    "          StructField(\"H21\", FloatType(), True),\n",
    "          StructField(\"H22\", FloatType(), True),\n",
    "          StructField(\"H23\", FloatType(), True)\n",
    "      ])\n",
    "   )\n",
    "             )])\n",
    "\n",
    "a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext.getOrCreate()\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"anomaly_detection\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "#load dataset\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema='true').load(DATASET_PATH+\"aggregated_dataset_rowBased.csv\")\n",
    "\n",
    "#cast\n",
    "def string_to_list(t):\n",
    "    return t.strip('][').split(', ') \n",
    "\n",
    "sdf.show()\n",
    "#sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- power: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n",
      "+---+-------------------+-----+--------------------+----+\n",
      "|num|               date|   id|               power|  V1|\n",
      "+---+-------------------+-----+--------------------+----+\n",
      "|  0|2014-08-15 00:00:00|Apt36|[1.96932, 0.38400...|true|\n",
      "|  1|2014-08-16 00:00:00|Apt36|[0.61410224,, 0.3...|true|\n",
      "+---+-------------------+-----+--------------------+----+\n",
      "\n",
      "+---+-------------------+-----+--------------------+\n",
      "|num|               date|   id|               power|\n",
      "+---+-------------------+-----+--------------------+\n",
      "|  0|2014-08-15 00:00:00|Apt36|[1.96932, 0.38400...|\n",
      "|  1|2014-08-16 00:00:00|Apt36|[0.61410224,, 0.3...|\n",
      "+---+-------------------+-----+--------------------+\n",
      "\n",
      "+---+-------------------+-----+----------+----------+----------+----------+---------+-----------+----------+-----------+----------+---------+---------+----------+----------+---------+----------+----------+----------+---------+----------+----------+---------+---------+---------+----------+\n",
      "|num|               date|   id|        H0|        H1|        H2|        H3|       H4|         H5|        H6|         H7|        H8|       H9|      H10|       H11|       H12|      H13|       H14|       H15|       H16|      H17|       H18|       H19|      H20|      H21|      H22|       H23|\n",
      "+---+-------------------+-----+----------+----------+----------+----------+---------+-----------+----------+-----------+----------+---------+---------+----------+----------+---------+----------+----------+----------+---------+----------+----------+---------+---------+---------+----------+\n",
      "|  0|2014-08-15 00:00:00|Apt36|   1.96932|0.38400444|0.36845776|0.37478444|0.7302378|0.024483334|0.97785443|0.024716666|0.73472667|1.5942433|0.8338022|0.02342222|0.72285223|1.0241567|0.77641886|0.78661555|0.27470222|1.4024155|0.29527557|0.14368889|0.8034489|1.1957567|0.2840011|0.33696333|\n",
      "|  1|2014-08-16 00:00:00|Apt36|0.61410224|      null| 0.3967911| 1.3392245|   0.5496|  1.6606333|      null|  1.2925533| 1.7467889|  1.74047|1.2520833| 1.7258222| 1.9947356|2.3601334| 1.4220456| 1.4096756| 0.8826689|2.5887167| 1.6094811| 1.9702544|  0.52544|0.5408767|1.3096111|0.44631332|\n",
      "+---+-------------------+-----+----------+----------+----------+----------+---------+-----------+----------+-----------+----------+---------+---------+----------+----------+---------+----------+----------+----------+---------+----------+----------+---------+---------+---------+----------+\n",
      "\n",
      "root\n",
      " |-- num: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- H0: float (nullable = true)\n",
      " |-- H1: float (nullable = true)\n",
      " |-- H2: float (nullable = true)\n",
      " |-- H3: float (nullable = true)\n",
      " |-- H4: float (nullable = true)\n",
      " |-- H5: float (nullable = true)\n",
      " |-- H6: float (nullable = true)\n",
      " |-- H7: float (nullable = true)\n",
      " |-- H8: float (nullable = true)\n",
      " |-- H9: float (nullable = true)\n",
      " |-- H10: float (nullable = true)\n",
      " |-- H11: float (nullable = true)\n",
      " |-- H12: float (nullable = true)\n",
      " |-- H13: float (nullable = true)\n",
      " |-- H14: float (nullable = true)\n",
      " |-- H15: float (nullable = true)\n",
      " |-- H16: float (nullable = true)\n",
      " |-- H17: float (nullable = true)\n",
      " |-- H18: float (nullable = true)\n",
      " |-- H19: float (nullable = true)\n",
      " |-- H20: float (nullable = true)\n",
      " |-- H21: float (nullable = true)\n",
      " |-- H22: float (nullable = true)\n",
      " |-- H23: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#c = sdf.withColumn('new_column',func_udf(sdf['_c0']))\n",
    "#c=sdf.withColumn(\"V\",f.when((size(sdf.power)==24) , True).otherwise(False))\n",
    "\n",
    "a = spark.read.format('csv').schema(schema).option(\"header\", \"true\").load(DATASET_PATH+\"f.csv\")\n",
    "\n",
    "def string_power_to_array(sdf):\n",
    "    temp=sdf.withColumn(\"power\", f.regexp_replace(f.regexp_replace(f.col(\"power\"), \"\\\\[\", \"\"),\"\\\\]\",\"\")\n",
    "                        .alias(\"power\"))\n",
    "    temp=temp.withColumn(\"power\",split(col(\"power\"), \",\\s*\")\n",
    "                        .cast(ArrayType(FloatType())).alias(\"power\"))\n",
    "    return temp\n",
    "\n",
    "a=string_power_to_array(a)\n",
    "\n",
    "a.printSchema()\n",
    "\n",
    "a=a.withColumn(\"V1\",f.when((size(a.power)==24) , True).otherwise(False))\n",
    "#a=a.withColumn(\"V2\",isnan(a.power))\n",
    "#a=a.withColumn(\"V3\",col(\"power\").isNull())\n",
    "#a=a.withColumn(\"V4\",a.power> 0)\n",
    "#a=a.withColumn(\"V5\",array_contains(col(\"power\"), 1.0))\n",
    "a.show()\n",
    "\n",
    "b=a.select(\"num\",\"date\",\"id\",\"power\")\n",
    "b.show()\n",
    "\n",
    "\n",
    "c=a.select(\"num\",\"date\",\"id\",\n",
    "           a.power[0].alias(\"H0\"), a.power[1].alias(\"H1\"), a.power[2].alias(\"H2\"),a.power[3].alias(\"H3\"),\n",
    "           a.power[4].alias(\"H4\"), a.power[5].alias(\"H5\"), a.power[6].alias(\"H6\"),a.power[7].alias(\"H7\"),\n",
    "           a.power[8].alias(\"H8\"), a.power[9].alias(\"H9\"), a.power[10].alias(\"H10\"),a.power[11].alias(\"H11\"),\n",
    "           a.power[12].alias(\"H12\"), a.power[13].alias(\"H13\"), a.power[14].alias(\"H14\"),a.power[15].alias(\"H15\"),\n",
    "           a.power[16].alias(\"H16\"), a.power[17].alias(\"H17\"), a.power[18].alias(\"H18\"),a.power[19].alias(\"H19\"),\n",
    "           a.power[20].alias(\"H20\"), a.power[21].alias(\"H21\"), a.power[22].alias(\"H22\"),a.power[23].alias(\"H23\")\n",
    "          )\n",
    "c.show()\n",
    "c.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMw6O+baIkYRsczOI9q7Cow",
   "collapsed_sections": [],
   "name": "Master Thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
